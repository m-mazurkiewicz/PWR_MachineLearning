{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mnist_2.1_five_layers_relu.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"XLrcPMI9hMNl","colab_type":"code","outputId":"33bfd70e-b539-4605-d92d-d37b70933037","colab":{"base_uri":"https://localhost:8080/","height":237},"executionInfo":{"status":"ok","timestamp":1541786935256,"user_tz":-60,"elapsed":8585,"user":{"displayName":"Maciej Falkiewicz","photoUrl":"","userId":"12525021891171762130"}}},"cell_type":"code","source":["!curl https://colab.chainer.org/install | sh -"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1379  100  1379    0     0  23372      0 --:--:-- --:--:-- --:--:-- 23775\n","+ apt -y -q install cuda-libraries-dev-9-2\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","cuda-libraries-dev-9-2 is already the newest version (9.2.148-1).\n","0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n","+ pip install -q cupy-cuda92  chainer \n","+ set +ex\n","Installation succeeded!\n"],"name":"stdout"}]},{"metadata":{"id":"3_n5ICc0hw33","colab_type":"code","outputId":"cef7bd88-ad13-4f93-d053-66509ba36b9c","colab":{"base_uri":"https://localhost:8080/","height":90}},"cell_type":"code","source":["!git clone https://github.com/MichalDanielDobrzanski/DeepLearningPython35"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'DeepLearningPython35'...\n","remote: Enumerating objects: 55, done.\u001b[K\n","remote: Total 55 (delta 0), reused 0 (delta 0), pack-reused 55\u001b[K\n","Unpacking objects: 100% (55/55), done.\n"],"name":"stdout"}]},{"metadata":{"id":"Huzc41d9ebnr","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-oEFMBTpi153","colab_type":"code","outputId":"28c6bb92-a0dd-49a8-c581-5f9b64ed98f1","colab":{"base_uri":"https://localhost:8080/","height":219}},"cell_type":"code","source":["!pip install autograd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting autograd\n","  Downloading https://files.pythonhosted.org/packages/08/7a/1ccee2a929d806ba3dbe632a196ad6a3f1423d6e261ae887e5fef2011420/autograd-1.2.tar.gz\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.6)\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n","Building wheels for collected packages: autograd\n","  Running setup.py bdist_wheel for autograd ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/72/6f/c2/40f130cca2c91f31d354bf72de282922479c09ce0b7853c4c5\n","Successfully built autograd\n","Installing collected packages: autograd\n","Successfully installed autograd-1.2\n"],"name":"stdout"}]},{"metadata":{"id":"JffYydjtb8ye","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from sklearn import preprocessing\n","from matplotlib import pyplot as plt\n","import autograd.numpy as np_autograd\n","from autograd import elementwise_grad as egrad\n","import cupy as cp\n","import pickle\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MFxa0eL5iJGQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# %load mnist_loader.py\n","\"\"\"\n","mnist_loader\n","~~~~~~~~~~~~\n","A library to load the MNIST image data.  For details of the data\n","structures that are returned, see the doc strings for ``load_data``\n","and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n","function usually called by our neural network code.\n","\"\"\"\n","\n","#### Libraries\n","# Standard library\n","import pickle\n","import gzip\n","\n","# Third-party libraries\n","import numpy as np\n","\n","def load_data():\n","    \"\"\"Return the MNIST data as a tuple containing the training data,\n","    the validation data, and the test data.\n","    The ``training_data`` is returned as a tuple with two entries.\n","    The first entry contains the actual training images.  This is a\n","    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n","    numpy ndarray with 784 values, representing the 28 * 28 = 784\n","    pixels in a single MNIST image.\n","    The second entry in the ``training_data`` tuple is a numpy ndarray\n","    containing 50,000 entries.  Those entries are just the digit\n","    values (0...9) for the corresponding images contained in the first\n","    entry of the tuple.\n","    The ``validation_data`` and ``test_data`` are similar, except\n","    each contains only 10,000 images.\n","    This is a nice data format, but for use in neural networks it's\n","    helpful to modify the format of the ``training_data`` a little.\n","    That's done in the wrapper function ``load_data_wrapper()``, see\n","    below.\n","    \"\"\"\n","    f = gzip.open('DeepLearningPython35/mnist.pkl.gz', 'rb')\n","    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n","    f.close()\n","    return (training_data, validation_data, test_data)\n","\n","def load_data_wrapper():\n","    \"\"\"Return a tuple containing ``(training_data, validation_data,\n","    test_data)``. Based on ``load_data``, but the format is more\n","    convenient for use in our implementation of neural networks.\n","    In particular, ``training_data`` is a list containing 50,000\n","    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n","    containing the input image.  ``y`` is a 10-dimensional\n","    numpy.ndarray representing the unit vector corresponding to the\n","    correct digit for ``x``.\n","    ``validation_data`` and ``test_data`` are lists containing 10,000\n","    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n","    numpy.ndarry containing the input image, and ``y`` is the\n","    corresponding classification, i.e., the digit values (integers)\n","    corresponding to ``x``.\n","    Obviously, this means we're using slightly different formats for\n","    the training data and the validation / test data.  These formats\n","    turn out to be the most convenient for use in our neural network\n","    code.\"\"\"\n","    tr_d, va_d, te_d = load_data()\n","    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n","    training_results = [vectorized_result(y) for y in tr_d[1]]\n","    training_data = (training_inputs, training_results) #zip(training_inputs, training_results)\n","    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n","    validation_data = (validation_inputs, [vectorized_result(y) for y in va_d[1]]) #zip(validation_inputs, va_d[1])\n","    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n","    test_data = (test_inputs, [vectorized_result(y) for y in te_d[1]]) #zip(test_inputs, te_d[1])\n","    return (training_data, validation_data, test_data)\n","\n","def vectorized_result(j):\n","    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n","    position and zeroes elsewhere.  This is used to convert a digit\n","    (0...9) into a corresponding desired output from the neural\n","    network.\"\"\"\n","    e = np.zeros((10, 1))\n","    e[j] = 1.0\n","    return e\n","\n","\n","def load_data_arrays():\n","    training_data, validation_data, test_data = load_data_wrapper()\n","    training_data_X, training_data_Y = training_data\n","    validation_data_X, validation_data_Y = validation_data\n","    training_data_X, training_data_Y = training_data\n","    training_data_X = cp.array(training_data_X)[:, :, 0].T\n","    training_data_Y = cp.array(training_data_Y)[:, :, 0].T\n","    validation_data_X, validation_data_Y = validation_data\n","    validation_data_X = cp.array(validation_data_X)[:, :, 0].T\n","    validation_data_Y = cp.array(validation_data_Y)[:, :, 0].T\n","    test_data_X, test_data_Y = test_data\n","    test_data_X = cp.array(test_data_X)[:, :, 0].T\n","    test_data_Y = cp.array(test_data_Y)[:, :, 0].T\n","    return training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MN-d_oj_iq4a","colab_type":"code","colab":{}},"cell_type":"code","source":["class NeuralNetwork:\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","\n","    def __init__(self, layers_size_vector, activation_function, cost_function='cross-entropy',\n","                 dropout_probabilities=None):\n","        self.number_of_layers = len(layers_size_vector)\n","        if type(activation_function) is list:\n","            if self.number_of_layers != len(activation_function) + 1:\n","                raise Exception(\"layer_size_vector & activation_function dimension mismatch\")\n","            self.activation_function = activation_function\n","        else:\n","            self.activation_function = [activation_function] * self.number_of_layers\n","        self.initialise_parameters(layers_size_vector)\n","        self.layers_size_vector = layers_size_vector\n","        self.cache = []\n","        self.fitted = False\n","        self.cost_function = cost_function\n","        if dropout_probabilities:\n","            if len(dropout_probabilities) != self.number_of_layers - 1:\n","                raise Exception(\"dropout_probabilities length & number_of_layers dimension mismatch\")\n","            self.dropout_probabilities = dict()\n","            for key, dropout_probability in enumerate(dropout_probabilities, 1):\n","                self.dropout_probabilities[key] = dropout_probability\n","        else:\n","            self.dropout_probabilities = dropout_probabilities\n","\n","    def initialise_parameters(self, layers_size_vector):\n","        self.weights = dict()\n","        self.bias = dict()\n","        for i in range(1, self.number_of_layers):\n","            self.weights[i] = cp.random.randn(layers_size_vector[i], layers_size_vector[i - 1]) * cp.sqrt(2 / layers_size_vector[i - 1])  # He initialisation\n","            # self.weights[i] = cp.random.randn(layers_size_vector[i],layers_size_vector[i-1]) / cp.sqrt(layers_size_vector[i-1])\n","            self.bias[i] = cp.zeros((layers_size_vector[i], 1))\n","\n","    def whole_output(self, A, dropout=False):\n","        self.cache_A = dict()\n","        self.cache_Z = dict()\n","        self.cache_A[0] = A\n","        for i in range(1, self.number_of_layers):\n","            Z = self.linear_forward(A, i)\n","            A = self.activation_function[i - 1](Z)\n","            if bool(self.dropout_probabilities) & dropout:\n","                if self.dropout_probabilities[i]!=0:\n","                    A = A * self.dropout_mask[i] / self.dropout_probabilities[i]\n","            self.cache_A[i] = A\n","            self.cache_Z[i] = Z\n","        return A\n","\n","    def linear_forward(self, previous_A, layer_no):\n","        return cp.dot(self.weights[layer_no], previous_A) + self.bias[layer_no]\n","\n","    def predict(self, input_matrix):\n","        output = self.whole_output(input_matrix)\n","        if output.shape[0] == 1:\n","            o = output > 0.5\n","            return o\n","        else:\n","            return np.argmax(output, axis=0)\n","\n","    def cost_function_evaluation(self, X, Y, _lambda=0):\n","        output = self.whole_output(X)\n","        if self.cost_function == 'cross-entropy':\n","            return cp.sum(-cp.multiply(Y, cp.log(output)) - cp.multiply((1 - Y), cp.log(1 - output))) / Y.shape[1]\n","        elif self.cost_function == 'euclidean_distance':\n","            return 1 / 2 * cp.sum(cp.power(Y - output, 2)) / Y.shape[1]\n","\n","    def output_layer_cost_derivative(self, output_matrix, Y):\n","        if self.cost_function == 'cross-entropy':\n","            return - (cp.divide(Y, output_matrix) - cp.divide(1 - Y, 1 - output_matrix))\n","        elif self.cost_function == 'euclidean_distance':\n","            return output_matrix - Y\n","        else:\n","            raise Exception(\"Wrong cost function name\")\n","\n","    def back_propagation(self, X, Y, regularisation_lambda=0):\n","        self.cost_derivatives = dict()\n","        self.weight_derivatives = dict()\n","        self.bias_derivatives = dict()\n","        dZ = self.output_layer_cost_derivative(self.whole_output(X), Y)\n","        for i in reversed(range(1, self.number_of_layers)):\n","            self.weight_derivatives[i] = (cp.dot(dZ, self.cache_A[i - 1].T) + regularisation_lambda * self.weights[i]) / X.shape[1]\n","            self.bias_derivatives[i] = cp.sum(dZ, axis=1, keepdims=True) / X.shape[1]\n","            self.cost_derivatives[i - 1] = cp.dot(self.weights[i].T, dZ)\n","            if i > 1:\n","                if self.dropout_probabilities:\n","                    if self.dropout_probabilities[i-1]:\n","                        self.cost_derivatives[i - 1] = self.cost_derivatives[i - 1] * self.dropout_mask[i - 1] / self.dropout_probabilities[i - 1]\n","                dZ = self.cost_derivatives[i - 1] * self.activation_function[i - 2](self.cache_A[i - 1], grad=True)\n","\n","    def update_weights(self, learning_rate):\n","        for i in range(1, self.number_of_layers):\n","            self.weights[i] -= learning_rate * self.weight_derivatives[i]\n","            self.bias[i] -= learning_rate * self.bias_derivatives[i]\n","\n","    def fit(self, X, Y, learning_rate, regularisation_lambda, epsilon, max_iteration_number=10000, min_max_normalization=False, validation_X = None, validation_Y = None):\n","        self.training_costs = []\n","        self.validation_costs = []\n","        if not self.fitted:\n","            if min_max_normalization:\n","                X = self.min_max_scaler.fit_transform(X) - 0.5\n","            previous_cost_function = float('inf')\n","            number_of_training_examples = X.shape[1]\n","            counter = 0\n","#             while ((self.cost_function_evaluation(X, Y) / previous_cost_function <= epsilon) and ( counter < max_iteration_number)):\n","            while counter<max_iteration_number:\n","                previous_cost_function = self.cost_function_evaluation(X, Y)\n","                if self.dropout_probabilities:\n","                    self.dropout(number_of_training_examples)\n","                self.whole_output(X, dropout=True)\n","                self.back_propagation(X, Y, regularisation_lambda)\n","                self.update_weights(learning_rate)\n","                counter += 1\n","                if counter % 10 == 0:\n","                    self.training_costs.append(self.cost_function_evaluation(X, Y))\n","                    if (validation_X is not None) and (validation_Y is not None):\n","                      self.validation_costs.append(self.cost_function_evaluation(validation_X, validation_Y))\n","                    print(\"Cost after iteration {}: {}\".format(counter, self.training_costs[-1]))\n","            self.fitted = True\n","            self.pickle_network(learning_rate, regularisation_lambda, max_iteration_number)\n","        else:\n","            raise Exception(\"Neural network already fitted!\")\n","\n","    def set_weights(self, list_of_parameters):\n","        for layer_no, parameters in enumerate(list_of_parameters):\n","            self.weights[layer_no] = parameters[0]\n","            self.bias[layer_no] = parameters[1]\n","            \n","    def pickle_network(self, learning_rate, regularisation_lambda, max_iteration_number):\n","      network_name = str()\n","      for i in range(1,self.number_of_layers):\n","        network_name += str(self.layers_size_vector[i])+self.activation_function[1].__name__ + '_'\n","      if self.dropout_probabilities:\n","        network_name += str(list(self.dropout_probabilities.values())) #'_'.join(str(self.dropout_probabilities))\n","      network_name += '_' + str(learning_rate)\n","      network_name += '_' + str(regularisation_lambda)\n","      network_name += '_' + str(max_iteration_number)\n","      with open('/content/gdrive/My Drive/'+network_name+'.pickle', 'wb') as f:\n","        pickle.dump(self,f)\n","\n","    def dropout(self, number_of_examples):\n","        self.dropout_mask = dict()\n","        for i in range(1,self.number_of_layers):\n","            if self.dropout_probabilities[i] != 0:\n","                self.dropout_mask[i] = cp.random.rand(self.layers_size_vector[i], number_of_examples) > self.dropout_probabilities[i]\n","\n","\n","def ReLU(x, grad=False):\n","    if grad:\n","      return x > 0\n","#         return cp.int64(x > 0)\n","    return x * (x > 0)\n","\n","\n","def sigmoid(x, grad=False):\n","    s = 1 / (1 + cp.exp(-x))\n","    if grad:\n","        return s * (1 - s)\n","    return s\n","\n","\n","def softmax(x, grad=False):\n","    def softmax_eval(x):\n","        e_x = np_autograd.exp(x - np_autograd.max(x))\n","        return e_x / e_x.sum(axis=0)\n","\n","    softmax_eval_grad = egrad(softmax_eval)\n","    if grad:\n","        return softmax_eval_grad(x)\n","    else:\n","        return softmax_eval(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c50d1rZyi9Jr","colab_type":"code","outputId":"6f2ffe88-6db6-41a5-d039-2c88cbd28299","colab":{"base_uri":"https://localhost:8080/","height":18632},"executionInfo":{"status":"ok","timestamp":1541798397898,"user_tz":-60,"elapsed":1160150,"user":{"displayName":"Maciej Falkiewicz","photoUrl":"","userId":"12525021891171762130"}}},"cell_type":"code","source":["activation_functions = [ReLU, ReLU, ReLU, ReLU, softmax]\n","layers_size_vector = [784, 200, 100, 60, 30, 10]\n","cost_function = 'euclidean_distance'\n","# cost_function = 'cross-entropy'\n","dropout_probabilities = None\n","learning_rate = .003\n","number_of_iterations = 10000\n","regularization_lambda = 0\n","training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y = load_data_arrays()\n","network = NeuralNetwork(layers_size_vector, activation_functions, cost_function, dropout_probabilities)\n","network.fit(training_data_X, training_data_Y, learning_rate, regularization_lambda, 1, number_of_iterations, validation_X = validation_data_X, validation_Y = validation_data_Y)\n","#network.fit(test_data_X, test_data_Y, learning_rate, regularization_lambda, 1, number_of_iterations)\n","plt.plot(network.training_costs, 'o')\n","plt.plot(network.validation_costs, 'ro')\n","plt.show()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Cost after iteration 10: 0.4603793235941576\n","Cost after iteration 20: 0.4544158903175791\n","Cost after iteration 30: 0.4504154677785298\n","Cost after iteration 40: 0.44737687339058085\n","Cost after iteration 50: 0.44486609153808065\n","Cost after iteration 60: 0.4426541069956057\n","Cost after iteration 70: 0.4406134154887103\n","Cost after iteration 80: 0.4386641315978418\n","Cost after iteration 90: 0.43675743848476123\n","Cost after iteration 100: 0.4348608602183804\n","Cost after iteration 110: 0.4329499661486082\n","Cost after iteration 120: 0.4310007189004877\n","Cost after iteration 130: 0.4290022342276525\n","Cost after iteration 140: 0.42694771813288135\n","Cost after iteration 150: 0.4248256335831125\n","Cost after iteration 160: 0.4226234279643359\n","Cost after iteration 170: 0.4203358722760711\n","Cost after iteration 180: 0.41795302421978825\n","Cost after iteration 190: 0.41546225100988826\n","Cost after iteration 200: 0.412858524509047\n","Cost after iteration 210: 0.4101413568901054\n","Cost after iteration 220: 0.4073080007118889\n","Cost after iteration 230: 0.4043584786017136\n","Cost after iteration 240: 0.40129055990034057\n","Cost after iteration 250: 0.398107144733935\n","Cost after iteration 260: 0.39482129139562633\n","Cost after iteration 270: 0.39144149338533624\n","Cost after iteration 280: 0.3879799986099233\n","Cost after iteration 290: 0.38444345530988583\n","Cost after iteration 300: 0.38084643765259546\n","Cost after iteration 310: 0.37719712454162974\n","Cost after iteration 320: 0.3735058487177716\n","Cost after iteration 330: 0.3697798787165431\n","Cost after iteration 340: 0.366027831370866\n","Cost after iteration 350: 0.36226110250278054\n","Cost after iteration 360: 0.35848745051245556\n","Cost after iteration 370: 0.3547103039181727\n","Cost after iteration 380: 0.35093537712189843\n","Cost after iteration 390: 0.3471662321879169\n","Cost after iteration 400: 0.34340463985701086\n","Cost after iteration 410: 0.3396493505009124\n","Cost after iteration 420: 0.3359047842121795\n","Cost after iteration 430: 0.3321723863702992\n","Cost after iteration 440: 0.32845004593645\n","Cost after iteration 450: 0.3247339268871039\n","Cost after iteration 460: 0.32102008866164145\n","Cost after iteration 470: 0.31731108774069444\n","Cost after iteration 480: 0.31360755531234863\n","Cost after iteration 490: 0.3099006954969334\n","Cost after iteration 500: 0.30618750306079906\n","Cost after iteration 510: 0.3024698182364419\n","Cost after iteration 520: 0.2987416756656118\n","Cost after iteration 530: 0.2949984054766061\n","Cost after iteration 540: 0.29124070342431285\n","Cost after iteration 550: 0.2874690074977268\n","Cost after iteration 560: 0.28369004737334025\n","Cost after iteration 570: 0.27989998836392443\n","Cost after iteration 580: 0.27609988910382033\n","Cost after iteration 590: 0.2722891622080143\n","Cost after iteration 600: 0.2684689019807264\n","Cost after iteration 610: 0.26464015397378277\n","Cost after iteration 620: 0.26080325882606703\n","Cost after iteration 630: 0.25696387676249627\n","Cost after iteration 640: 0.2531226041120541\n","Cost after iteration 650: 0.24928623645905879\n","Cost after iteration 660: 0.2454563407306742\n","Cost after iteration 670: 0.241640526048756\n","Cost after iteration 680: 0.23783596624598963\n","Cost after iteration 690: 0.23405012382814136\n","Cost after iteration 700: 0.23028727853265235\n","Cost after iteration 710: 0.22655238053003296\n","Cost after iteration 720: 0.22285637236490533\n","Cost after iteration 730: 0.2192068651752875\n","Cost after iteration 740: 0.215606434143002\n","Cost after iteration 750: 0.2120632488169172\n","Cost after iteration 760: 0.20858154733333012\n","Cost after iteration 770: 0.2051695150856592\n","Cost after iteration 780: 0.20183387129253566\n","Cost after iteration 790: 0.19858089754593383\n","Cost after iteration 800: 0.1954092777665791\n","Cost after iteration 810: 0.19232030279249607\n","Cost after iteration 820: 0.18931817876555174\n","Cost after iteration 830: 0.18640353293620748\n","Cost after iteration 840: 0.1835786646572615\n","Cost after iteration 850: 0.18083987855476644\n","Cost after iteration 860: 0.178185449809032\n","Cost after iteration 870: 0.1756216816314262\n","Cost after iteration 880: 0.1731465072278084\n","Cost after iteration 890: 0.17075639582216712\n","Cost after iteration 900: 0.16844442467831625\n","Cost after iteration 910: 0.16621323289461806\n","Cost after iteration 920: 0.16405729335964184\n","Cost after iteration 930: 0.16197021189339503\n","Cost after iteration 940: 0.1599549954102228\n","Cost after iteration 950: 0.1580091582791363\n","Cost after iteration 960: 0.1561273106718144\n","Cost after iteration 970: 0.15430556677941615\n","Cost after iteration 980: 0.1525436377540018\n","Cost after iteration 990: 0.1508355504939547\n","Cost after iteration 1000: 0.14918013548621134\n","Cost after iteration 1010: 0.14757685836559856\n","Cost after iteration 1020: 0.14602501007166435\n","Cost after iteration 1030: 0.14451951142143196\n","Cost after iteration 1040: 0.14305750581781132\n","Cost after iteration 1050: 0.14163798462433483\n","Cost after iteration 1060: 0.14025874257704843\n","Cost after iteration 1070: 0.13891986604506631\n","Cost after iteration 1080: 0.1376174922918068\n","Cost after iteration 1090: 0.13635172509077823\n","Cost after iteration 1100: 0.13512108065147746\n","Cost after iteration 1110: 0.1339243710047787\n","Cost after iteration 1120: 0.13276066353255092\n","Cost after iteration 1130: 0.13162858525558252\n","Cost after iteration 1140: 0.13052582121883585\n","Cost after iteration 1150: 0.12945082751735099\n","Cost after iteration 1160: 0.12840397623598176\n","Cost after iteration 1170: 0.1273811376638338\n","Cost after iteration 1180: 0.1263828589513914\n","Cost after iteration 1190: 0.12540831721433296\n","Cost after iteration 1200: 0.12445699770995958\n","Cost after iteration 1210: 0.12352879396018321\n","Cost after iteration 1220: 0.12262377962795618\n","Cost after iteration 1230: 0.12174003872407488\n","Cost after iteration 1240: 0.1208769966551026\n","Cost after iteration 1250: 0.12003382473161073\n","Cost after iteration 1260: 0.11920846864624436\n","Cost after iteration 1270: 0.11840285195885562\n","Cost after iteration 1280: 0.1176146247874575\n","Cost after iteration 1290: 0.11684456626556824\n","Cost after iteration 1300: 0.11609149900914509\n","Cost after iteration 1310: 0.1153545706719545\n","Cost after iteration 1320: 0.11463321626777626\n","Cost after iteration 1330: 0.11392744221567513\n","Cost after iteration 1340: 0.1132371871278417\n","Cost after iteration 1350: 0.11256133822866293\n","Cost after iteration 1360: 0.11189910089973691\n","Cost after iteration 1370: 0.11125089062338961\n","Cost after iteration 1380: 0.11061517316385266\n","Cost after iteration 1390: 0.10999197375550744\n","Cost after iteration 1400: 0.10938121734216832\n","Cost after iteration 1410: 0.10878268292236479\n","Cost after iteration 1420: 0.10819614840466249\n","Cost after iteration 1430: 0.10762132634956253\n","Cost after iteration 1440: 0.10705730429553087\n","Cost after iteration 1450: 0.10650331453175241\n","Cost after iteration 1460: 0.10595967279916017\n","Cost after iteration 1470: 0.1054261080676065\n","Cost after iteration 1480: 0.10490259864470822\n","Cost after iteration 1490: 0.10438880473644271\n","Cost after iteration 1500: 0.10388431356960638\n","Cost after iteration 1510: 0.10338917579987814\n","Cost after iteration 1520: 0.10290319734715442\n","Cost after iteration 1530: 0.10242536283997543\n","Cost after iteration 1540: 0.10195555169392934\n","Cost after iteration 1550: 0.10149396427686143\n","Cost after iteration 1560: 0.10104031064535951\n","Cost after iteration 1570: 0.10059434210946495\n","Cost after iteration 1580: 0.10015546785357998\n","Cost after iteration 1590: 0.0997237077121084\n","Cost after iteration 1600: 0.09929911576843177\n","Cost after iteration 1610: 0.09888094433114922\n","Cost after iteration 1620: 0.09846981025421878\n","Cost after iteration 1630: 0.09806533028298065\n","Cost after iteration 1640: 0.09766752597478004\n","Cost after iteration 1650: 0.09727627706363728\n","Cost after iteration 1660: 0.09689096286960752\n","Cost after iteration 1670: 0.0965117406427737\n","Cost after iteration 1680: 0.096138177894416\n","Cost after iteration 1690: 0.09576975622511626\n","Cost after iteration 1700: 0.09540693877189019\n","Cost after iteration 1710: 0.0950492615176778\n","Cost after iteration 1720: 0.09469700285210522\n","Cost after iteration 1730: 0.09435008913393984\n","Cost after iteration 1740: 0.09400825040665266\n","Cost after iteration 1750: 0.09367112953789103\n","Cost after iteration 1760: 0.09333836923279512\n","Cost after iteration 1770: 0.09301022500412601\n","Cost after iteration 1780: 0.09268641268431618\n","Cost after iteration 1790: 0.09236669990550478\n","Cost after iteration 1800: 0.09205165315589402\n","Cost after iteration 1810: 0.0917409689497215\n","Cost after iteration 1820: 0.0914344449055471\n","Cost after iteration 1830: 0.09113266780980885\n","Cost after iteration 1840: 0.09083484204219215\n","Cost after iteration 1850: 0.09054137343006557\n","Cost after iteration 1860: 0.09025141031072231\n","Cost after iteration 1870: 0.08996490035453773\n","Cost after iteration 1880: 0.08968238398560872\n","Cost after iteration 1890: 0.0894035923827031\n","Cost after iteration 1900: 0.08912781455426737\n","Cost after iteration 1910: 0.08885514593483529\n","Cost after iteration 1920: 0.08858616770131596\n","Cost after iteration 1930: 0.08832051680801577\n","Cost after iteration 1940: 0.08805795508123175\n","Cost after iteration 1950: 0.0877982349199304\n","Cost after iteration 1960: 0.08754142749410138\n","Cost after iteration 1970: 0.08728770634321205\n","Cost after iteration 1980: 0.08703701394242096\n","Cost after iteration 1990: 0.08678912165785238\n","Cost after iteration 2000: 0.08654429533628809\n","Cost after iteration 2010: 0.08630234410046796\n","Cost after iteration 2020: 0.08606310947706099\n","Cost after iteration 2030: 0.08582651622595007\n","Cost after iteration 2040: 0.08559271426277261\n","Cost after iteration 2050: 0.08536167978003023\n","Cost after iteration 2060: 0.08513309230291168\n","Cost after iteration 2070: 0.08490714189202633\n","Cost after iteration 2080: 0.08468319841426018\n","Cost after iteration 2090: 0.08446166129368732\n","Cost after iteration 2100: 0.08424247504728563\n","Cost after iteration 2110: 0.08402593487657462\n","Cost after iteration 2120: 0.08381160756021261\n","Cost after iteration 2130: 0.08359895196446154\n","Cost after iteration 2140: 0.0833885329131173\n","Cost after iteration 2150: 0.0831807823345437\n","Cost after iteration 2160: 0.08297512836071208\n","Cost after iteration 2170: 0.08277178993174567\n","Cost after iteration 2180: 0.08257045689949871\n","Cost after iteration 2190: 0.08237110434443566\n","Cost after iteration 2200: 0.08217366778499789\n","Cost after iteration 2210: 0.08197783190242333\n","Cost after iteration 2220: 0.08178357634792255\n","Cost after iteration 2230: 0.08159118911234602\n","Cost after iteration 2240: 0.081400750122681\n","Cost after iteration 2250: 0.08121214462689813\n","Cost after iteration 2260: 0.08102541494085187\n","Cost after iteration 2270: 0.08084046297305927\n","Cost after iteration 2280: 0.08065715953267202\n","Cost after iteration 2290: 0.0804755784544579\n","Cost after iteration 2300: 0.08029560358172734\n","Cost after iteration 2310: 0.08011717524508843\n","Cost after iteration 2320: 0.07994001429078115\n","Cost after iteration 2330: 0.07976443762760452\n","Cost after iteration 2340: 0.07959064414808348\n","Cost after iteration 2350: 0.07941852684296544\n","Cost after iteration 2360: 0.07924769108818717\n","Cost after iteration 2370: 0.07907810834548944\n","Cost after iteration 2380: 0.07891006779113564\n","Cost after iteration 2390: 0.07874337079245036\n","Cost after iteration 2400: 0.0785783238779784\n","Cost after iteration 2410: 0.07841442627503283\n","Cost after iteration 2420: 0.07825177180930244\n","Cost after iteration 2430: 0.07809072249798733\n","Cost after iteration 2440: 0.07793122364582608\n","Cost after iteration 2450: 0.07777302588481974\n","Cost after iteration 2460: 0.07761616057927749\n","Cost after iteration 2470: 0.07746089191243855\n","Cost after iteration 2480: 0.07730675306372949\n","Cost after iteration 2490: 0.07715336060328211\n","Cost after iteration 2500: 0.07700116108138777\n","Cost after iteration 2510: 0.07685015082140538\n","Cost after iteration 2520: 0.07670038688629785\n","Cost after iteration 2530: 0.07655172580493351\n","Cost after iteration 2540: 0.07640425046308347\n","Cost after iteration 2550: 0.07625789329517067\n","Cost after iteration 2560: 0.07611255811873223\n","Cost after iteration 2570: 0.07596821184306547\n","Cost after iteration 2580: 0.07582484403204189\n","Cost after iteration 2590: 0.07568254748168804\n","Cost after iteration 2600: 0.07554128641353278\n","Cost after iteration 2610: 0.07540088332163103\n","Cost after iteration 2620: 0.07526168540788374\n","Cost after iteration 2630: 0.07512362983967566\n","Cost after iteration 2640: 0.07498660314493205\n","Cost after iteration 2650: 0.07485049516769854\n","Cost after iteration 2660: 0.07471555794646431\n","Cost after iteration 2670: 0.07458167005125406\n","Cost after iteration 2680: 0.07444874999437692\n","Cost after iteration 2690: 0.07431682686425213\n","Cost after iteration 2700: 0.07418595815952664\n","Cost after iteration 2710: 0.07405597225202677\n","Cost after iteration 2720: 0.0739267686582425\n","Cost after iteration 2730: 0.07379857472419901\n","Cost after iteration 2740: 0.0736711808072817\n","Cost after iteration 2750: 0.07354444594242647\n","Cost after iteration 2760: 0.07341842028770791\n","Cost after iteration 2770: 0.07329331269415638\n","Cost after iteration 2780: 0.0731689018326028\n","Cost after iteration 2790: 0.07304542525960994\n","Cost after iteration 2800: 0.07292273089226173\n","Cost after iteration 2810: 0.07280065736983664\n","Cost after iteration 2820: 0.07267940669294039\n","Cost after iteration 2830: 0.07255906606492238\n","Cost after iteration 2840: 0.07243959781640137\n","Cost after iteration 2850: 0.07232081026729806\n","Cost after iteration 2860: 0.07220254894713923\n","Cost after iteration 2870: 0.07208493979751028\n","Cost after iteration 2880: 0.07196791717557262\n","Cost after iteration 2890: 0.07185161874559579\n","Cost after iteration 2900: 0.07173598104227874\n","Cost after iteration 2910: 0.07162079904132417\n","Cost after iteration 2920: 0.07150638116090052\n","Cost after iteration 2930: 0.07139282376116071\n","Cost after iteration 2940: 0.07127999096091024\n","Cost after iteration 2950: 0.07116803292990617\n","Cost after iteration 2960: 0.07105676969252496\n","Cost after iteration 2970: 0.07094624515531987\n","Cost after iteration 2980: 0.0708364032311289\n","Cost after iteration 2990: 0.07072689624319173\n","Cost after iteration 3000: 0.07061783713311942\n","Cost after iteration 3010: 0.0705096359955173\n","Cost after iteration 3020: 0.0704020677098982\n","Cost after iteration 3030: 0.07029506651090564\n","Cost after iteration 3040: 0.07018836926254392\n","Cost after iteration 3050: 0.07008227982280495\n","Cost after iteration 3060: 0.06997680557499192\n","Cost after iteration 3070: 0.06987191337672373\n","Cost after iteration 3080: 0.06976770240576\n","Cost after iteration 3090: 0.06966393222955859\n","Cost after iteration 3100: 0.06956061286095186\n","Cost after iteration 3110: 0.06945773293797171\n","Cost after iteration 3120: 0.06935535983177739\n","Cost after iteration 3130: 0.0692536217351716\n","Cost after iteration 3140: 0.06915254887162006\n","Cost after iteration 3150: 0.06905177589878903\n","Cost after iteration 3160: 0.06895147143884435\n","Cost after iteration 3170: 0.06885179578755457\n","Cost after iteration 3180: 0.06875270307599837\n","Cost after iteration 3190: 0.06865413655032293\n","Cost after iteration 3200: 0.06855602488269513\n","Cost after iteration 3210: 0.06845827311315214\n","Cost after iteration 3220: 0.06836110849145846\n","Cost after iteration 3230: 0.06826442214434546\n","Cost after iteration 3240: 0.06816817481449373\n","Cost after iteration 3250: 0.06807234753257534\n","Cost after iteration 3260: 0.06797710653650627\n","Cost after iteration 3270: 0.06788247278703989\n","Cost after iteration 3280: 0.06778821682923981\n","Cost after iteration 3290: 0.06769451837500032\n","Cost after iteration 3300: 0.06760118694969613\n","Cost after iteration 3310: 0.06750810617754874\n","Cost after iteration 3320: 0.06741535321404213\n","Cost after iteration 3330: 0.06732316755393983\n","Cost after iteration 3340: 0.06723136427721521\n","Cost after iteration 3350: 0.06714021464730963\n","Cost after iteration 3360: 0.06704957074643346\n","Cost after iteration 3370: 0.06695941146941732\n","Cost after iteration 3380: 0.06686961550429817\n","Cost after iteration 3390: 0.06678021074579615\n","Cost after iteration 3400: 0.06669132345266376\n","Cost after iteration 3410: 0.06660263852554994\n","Cost after iteration 3420: 0.06651433245551606\n","Cost after iteration 3430: 0.06642627766516872\n","Cost after iteration 3440: 0.06633851379078369\n","Cost after iteration 3450: 0.0662512958551855\n","Cost after iteration 3460: 0.06616455346078672\n","Cost after iteration 3470: 0.06607805757886033\n","Cost after iteration 3480: 0.06599197599010448\n","Cost after iteration 3490: 0.06590592503192268\n","Cost after iteration 3500: 0.06582009051742153\n","Cost after iteration 3510: 0.06573450437708804\n","Cost after iteration 3520: 0.06564922769130212\n","Cost after iteration 3530: 0.06556442480086444\n","Cost after iteration 3540: 0.06548022090935027\n","Cost after iteration 3550: 0.0653962525873312\n","Cost after iteration 3560: 0.0653127101726771\n","Cost after iteration 3570: 0.06522959188327328\n","Cost after iteration 3580: 0.06514669102161137\n","Cost after iteration 3590: 0.06506417966333826\n","Cost after iteration 3600: 0.06498181029863222\n","Cost after iteration 3610: 0.06489992971453835\n","Cost after iteration 3620: 0.06481832868304546\n","Cost after iteration 3630: 0.06473714047328558\n","Cost after iteration 3640: 0.06465627718159547\n","Cost after iteration 3650: 0.06457566044587242\n","Cost after iteration 3660: 0.0644955132989486\n","Cost after iteration 3670: 0.06441568681424267\n","Cost after iteration 3680: 0.06433612498674816\n","Cost after iteration 3690: 0.06425692024692296\n","Cost after iteration 3700: 0.06417813698197988\n","Cost after iteration 3710: 0.06409962387421282\n","Cost after iteration 3720: 0.0640212835540143\n","Cost after iteration 3730: 0.06394318182334748\n","Cost after iteration 3740: 0.06386539862481064\n","Cost after iteration 3750: 0.06378793036911987\n","Cost after iteration 3760: 0.06371094429556379\n","Cost after iteration 3770: 0.06363433798881085\n","Cost after iteration 3780: 0.06355795671747363\n","Cost after iteration 3790: 0.06348181554784951\n","Cost after iteration 3800: 0.0634058831710503\n","Cost after iteration 3810: 0.06333028358824154\n","Cost after iteration 3820: 0.06325493799879607\n","Cost after iteration 3830: 0.06317980975329754\n","Cost after iteration 3840: 0.063105108694261\n","Cost after iteration 3850: 0.06303063949525771\n","Cost after iteration 3860: 0.06295642240107069\n","Cost after iteration 3870: 0.06288241179626979\n","Cost after iteration 3880: 0.06280862178889209\n","Cost after iteration 3890: 0.0627350639255794\n","Cost after iteration 3900: 0.0626618206597671\n","Cost after iteration 3910: 0.06258890724980101\n","Cost after iteration 3920: 0.0625163580890299\n","Cost after iteration 3930: 0.06244398120962445\n","Cost after iteration 3940: 0.06237178346592435\n","Cost after iteration 3950: 0.06229969256562014\n","Cost after iteration 3960: 0.06222778072238829\n","Cost after iteration 3970: 0.062156202942397286\n","Cost after iteration 3980: 0.06208491943542599\n","Cost after iteration 3990: 0.062013845381055234\n","Cost after iteration 4000: 0.06194278970205355\n","Cost after iteration 4010: 0.06187199198310931\n","Cost after iteration 4020: 0.061801540604423374\n","Cost after iteration 4030: 0.061731219841075755\n","Cost after iteration 4040: 0.06166097608880645\n","Cost after iteration 4050: 0.06159116744996591\n","Cost after iteration 4060: 0.06152132586030706\n","Cost after iteration 4070: 0.061451849517156404\n","Cost after iteration 4080: 0.06138260529112877\n","Cost after iteration 4090: 0.06131371648036223\n","Cost after iteration 4100: 0.06124504736549021\n","Cost after iteration 4110: 0.06117664999883304\n","Cost after iteration 4120: 0.06110850801411505\n","Cost after iteration 4130: 0.06104058474125342\n","Cost after iteration 4140: 0.06097271348917788\n","Cost after iteration 4150: 0.06090523494681869\n","Cost after iteration 4160: 0.0608379820455902\n","Cost after iteration 4170: 0.060770959872734676\n","Cost after iteration 4180: 0.06070422956261995\n","Cost after iteration 4190: 0.06063757777257824\n","Cost after iteration 4200: 0.060571065474144155\n","Cost after iteration 4210: 0.06050486211603562\n","Cost after iteration 4220: 0.06043881681786418\n","Cost after iteration 4230: 0.06037309005960163\n","Cost after iteration 4240: 0.06030768981820587\n","Cost after iteration 4250: 0.06024248115040084\n","Cost after iteration 4260: 0.060177539033258017\n","Cost after iteration 4270: 0.060112858491584154\n","Cost after iteration 4280: 0.06004826222814491\n","Cost after iteration 4290: 0.05998377068828387\n","Cost after iteration 4300: 0.05991935594731822\n","Cost after iteration 4310: 0.05985519995960528\n","Cost after iteration 4320: 0.059791236999311694\n","Cost after iteration 4330: 0.05972736323910591\n","Cost after iteration 4340: 0.05966371329030276\n","Cost after iteration 4350: 0.059600177416559166\n","Cost after iteration 4360: 0.059536891582871294\n","Cost after iteration 4370: 0.059474046387610606\n","Cost after iteration 4380: 0.05941135004186987\n","Cost after iteration 4390: 0.059348918176824174\n","Cost after iteration 4400: 0.05928662533560093\n","Cost after iteration 4410: 0.05922453079744963\n","Cost after iteration 4420: 0.059162538013039494\n","Cost after iteration 4430: 0.05910073951346418\n","Cost after iteration 4440: 0.059039022109722485\n","Cost after iteration 4450: 0.05897765586953475\n","Cost after iteration 4460: 0.05891637546215341\n","Cost after iteration 4470: 0.058855235670624215\n","Cost after iteration 4480: 0.05879420428220054\n","Cost after iteration 4490: 0.05873324840024987\n","Cost after iteration 4500: 0.05867256678396416\n","Cost after iteration 4510: 0.05861196100042473\n","Cost after iteration 4520: 0.058551596360023356\n","Cost after iteration 4530: 0.05849132743558475\n","Cost after iteration 4540: 0.05843127946109751\n","Cost after iteration 4550: 0.05837146520446697\n","Cost after iteration 4560: 0.058311787646111043\n","Cost after iteration 4570: 0.058252100345190176\n","Cost after iteration 4580: 0.05819257549543064\n","Cost after iteration 4590: 0.05813322340512512\n","Cost after iteration 4600: 0.05807410606925978\n","Cost after iteration 4610: 0.05801518510392897\n","Cost after iteration 4620: 0.057956445880933245\n","Cost after iteration 4630: 0.057897674137617\n","Cost after iteration 4640: 0.057839074586717734\n","Cost after iteration 4650: 0.05778069508718881\n","Cost after iteration 4660: 0.05772243860848533\n","Cost after iteration 4670: 0.05766436815727147\n","Cost after iteration 4680: 0.05760642354881162\n","Cost after iteration 4690: 0.057548574512834176\n","Cost after iteration 4700: 0.05749093227270191\n","Cost after iteration 4710: 0.05743348598191733\n","Cost after iteration 4720: 0.05737621847108613\n","Cost after iteration 4730: 0.057319247302901846\n","Cost after iteration 4740: 0.05726249455745814\n","Cost after iteration 4750: 0.057205988826684824\n","Cost after iteration 4760: 0.05714949286310685\n","Cost after iteration 4770: 0.057093158992147265\n","Cost after iteration 4780: 0.05703697394338508\n","Cost after iteration 4790: 0.056980911104119056\n","Cost after iteration 4800: 0.05692479056817472\n","Cost after iteration 4810: 0.056868811995497655\n","Cost after iteration 4820: 0.0568130681742989\n","Cost after iteration 4830: 0.056757505291420866\n","Cost after iteration 4840: 0.05670201840271614\n","Cost after iteration 4850: 0.05664663413101337\n","Cost after iteration 4860: 0.056591438675508164\n","Cost after iteration 4870: 0.056536373945706944\n","Cost after iteration 4880: 0.056481390442274494\n","Cost after iteration 4890: 0.05642660880951582\n","Cost after iteration 4900: 0.05637210136897601\n","Cost after iteration 4910: 0.05631780592460029\n","Cost after iteration 4920: 0.0562635995774804\n","Cost after iteration 4930: 0.05620949825778578\n","Cost after iteration 4940: 0.056155668946722234\n","Cost after iteration 4950: 0.05610198622068832\n","Cost after iteration 4960: 0.05604844445092421\n","Cost after iteration 4970: 0.055995004993705313\n","Cost after iteration 4980: 0.055941598792397414\n","Cost after iteration 4990: 0.055888279852813104\n","Cost after iteration 5000: 0.05583508604468526\n","Cost after iteration 5010: 0.05578200478768952\n","Cost after iteration 5020: 0.055728977922526436\n","Cost after iteration 5030: 0.05567600331736305\n","Cost after iteration 5040: 0.05562324481699336\n","Cost after iteration 5050: 0.05557070008524037\n","Cost after iteration 5060: 0.05551833756275977\n","Cost after iteration 5070: 0.05546611344876401\n","Cost after iteration 5080: 0.05541404658896261\n","Cost after iteration 5090: 0.055362027739072533\n","Cost after iteration 5100: 0.05531004723486588\n","Cost after iteration 5110: 0.055258332284692825\n","Cost after iteration 5120: 0.055206627574662506\n","Cost after iteration 5130: 0.05515515475424105\n","Cost after iteration 5140: 0.055103759370987\n","Cost after iteration 5150: 0.05505250175119711\n","Cost after iteration 5160: 0.05500143686311625\n","Cost after iteration 5170: 0.054950468049669535\n","Cost after iteration 5180: 0.0548995579887927\n","Cost after iteration 5190: 0.054848844404746216\n","Cost after iteration 5200: 0.05479820478455791\n","Cost after iteration 5210: 0.05474774111536912\n","Cost after iteration 5220: 0.054697346192065045\n","Cost after iteration 5230: 0.054646936039698696\n","Cost after iteration 5240: 0.054596689808264015\n","Cost after iteration 5250: 0.05454658359801064\n","Cost after iteration 5260: 0.05449656315633033\n","Cost after iteration 5270: 0.05444668984224459\n","Cost after iteration 5280: 0.054397062431968735\n","Cost after iteration 5290: 0.05434737948254191\n","Cost after iteration 5300: 0.0542978519935658\n","Cost after iteration 5310: 0.05424855877062969\n","Cost after iteration 5320: 0.05419937578295025\n","Cost after iteration 5330: 0.054150325350102155\n","Cost after iteration 5340: 0.05410122758781429\n","Cost after iteration 5350: 0.05405220755557758\n","Cost after iteration 5360: 0.054003360736063356\n","Cost after iteration 5370: 0.05395456119644706\n","Cost after iteration 5380: 0.053905910091323055\n","Cost after iteration 5390: 0.053857492237335006\n","Cost after iteration 5400: 0.053809035708117445\n","Cost after iteration 5410: 0.05376060317011674\n","Cost after iteration 5420: 0.05371216870673574\n","Cost after iteration 5430: 0.05366380892926927\n","Cost after iteration 5440: 0.05361556676886979\n","Cost after iteration 5450: 0.053567502287924014\n","Cost after iteration 5460: 0.05351956832907556\n","Cost after iteration 5470: 0.053471735254889644\n","Cost after iteration 5480: 0.05342396359227176\n","Cost after iteration 5490: 0.05337629525558723\n","Cost after iteration 5500: 0.05332885764927052\n","Cost after iteration 5510: 0.05328161616522465\n","Cost after iteration 5520: 0.05323441952384036\n","Cost after iteration 5530: 0.053187354063437435\n","Cost after iteration 5540: 0.05314041292024258\n","Cost after iteration 5550: 0.05309354841262845\n","Cost after iteration 5560: 0.05304671492818399\n","Cost after iteration 5570: 0.05299993386547011\n","Cost after iteration 5580: 0.05295309702559424\n","Cost after iteration 5590: 0.05290639992780807\n","Cost after iteration 5600: 0.05285986732522099\n","Cost after iteration 5610: 0.05281343876789528\n","Cost after iteration 5620: 0.0527670776279443\n","Cost after iteration 5630: 0.05272082540621277\n","Cost after iteration 5640: 0.05267474140963021\n","Cost after iteration 5650: 0.05262883194706397\n","Cost after iteration 5660: 0.05258309329137866\n","Cost after iteration 5670: 0.05253741492797356\n","Cost after iteration 5680: 0.05249178690818562\n","Cost after iteration 5690: 0.05244629579426341\n","Cost after iteration 5700: 0.052400823135328935\n","Cost after iteration 5710: 0.052355486381168236\n","Cost after iteration 5720: 0.052310373770060134\n","Cost after iteration 5730: 0.05226538048013932\n","Cost after iteration 5740: 0.05222048706446156\n","Cost after iteration 5750: 0.052175714034845756\n","Cost after iteration 5760: 0.05213092026126713\n","Cost after iteration 5770: 0.05208611000576302\n","Cost after iteration 5780: 0.052041440893524395\n","Cost after iteration 5790: 0.05199679856098\n","Cost after iteration 5800: 0.05195224971074689\n","Cost after iteration 5810: 0.05190779516471976\n","Cost after iteration 5820: 0.05186351138569615\n","Cost after iteration 5830: 0.05181936653037945\n","Cost after iteration 5840: 0.05177535855992643\n","Cost after iteration 5850: 0.05173138343290653\n","Cost after iteration 5860: 0.051687482927030945\n","Cost after iteration 5870: 0.051643678498469237\n","Cost after iteration 5880: 0.0515998902703576\n","Cost after iteration 5890: 0.05155617074005611\n","Cost after iteration 5900: 0.05151262414051698\n","Cost after iteration 5910: 0.05146914493615252\n","Cost after iteration 5920: 0.05142573735451946\n","Cost after iteration 5930: 0.05138243288054773\n","Cost after iteration 5940: 0.051339225140420525\n","Cost after iteration 5950: 0.05129614189366787\n","Cost after iteration 5960: 0.05125309586427876\n","Cost after iteration 5970: 0.05121024735359395\n","Cost after iteration 5980: 0.051167354573868486\n","Cost after iteration 5990: 0.051124419874580815\n","Cost after iteration 6000: 0.0510814609499341\n","Cost after iteration 6010: 0.051038760604838224\n","Cost after iteration 6020: 0.05099628744872347\n","Cost after iteration 6030: 0.0509538360059246\n","Cost after iteration 6040: 0.05091149003155902\n","Cost after iteration 6050: 0.05086902831672109\n","Cost after iteration 6060: 0.05082687865854836\n","Cost after iteration 6070: 0.05078486429058754\n","Cost after iteration 6080: 0.05074284042309762\n","Cost after iteration 6090: 0.05070092813446803\n","Cost after iteration 6100: 0.05065908671624408\n","Cost after iteration 6110: 0.05061727468167348\n","Cost after iteration 6120: 0.050575383435042656\n","Cost after iteration 6130: 0.0505336480379381\n","Cost after iteration 6140: 0.05049202998367542\n","Cost after iteration 6150: 0.05045055999770014\n","Cost after iteration 6160: 0.050409277858242375\n","Cost after iteration 6170: 0.05036803465450108\n","Cost after iteration 6180: 0.050326770474179786\n","Cost after iteration 6190: 0.050285662667090954\n","Cost after iteration 6200: 0.050244605005595966\n","Cost after iteration 6210: 0.05020354920955262\n","Cost after iteration 6220: 0.050162536468107526\n","Cost after iteration 6230: 0.05012163031414552\n","Cost after iteration 6240: 0.05008088797271802\n","Cost after iteration 6250: 0.05004020103511284\n","Cost after iteration 6260: 0.0499996378229233\n","Cost after iteration 6270: 0.049959206777877935\n","Cost after iteration 6280: 0.04991889501819069\n","Cost after iteration 6290: 0.04987858616670732\n","Cost after iteration 6300: 0.04983832527315854\n","Cost after iteration 6310: 0.04979825418610988\n","Cost after iteration 6320: 0.049758288528345296\n","Cost after iteration 6330: 0.04971834663852738\n","Cost after iteration 6340: 0.049678584326418404\n","Cost after iteration 6350: 0.049638856359028205\n","Cost after iteration 6360: 0.04959917283786757\n","Cost after iteration 6370: 0.04955953670560743\n","Cost after iteration 6380: 0.04952003198297211\n","Cost after iteration 6390: 0.049480638654470374\n","Cost after iteration 6400: 0.049441360254031924\n","Cost after iteration 6410: 0.04940208501718752\n","Cost after iteration 6420: 0.04936287048057503\n","Cost after iteration 6430: 0.04932380249683572\n","Cost after iteration 6440: 0.04928468335231329\n","Cost after iteration 6450: 0.04924560714946205\n","Cost after iteration 6460: 0.049206597314835746\n","Cost after iteration 6470: 0.049167635935790065\n","Cost after iteration 6480: 0.04912882154175489\n","Cost after iteration 6490: 0.04909001726499245\n","Cost after iteration 6500: 0.049051333480850855\n","Cost after iteration 6510: 0.049012790919048366\n","Cost after iteration 6520: 0.048974302475849066\n","Cost after iteration 6530: 0.048935950902657295\n","Cost after iteration 6540: 0.048897579729734374\n","Cost after iteration 6550: 0.048859307317573895\n","Cost after iteration 6560: 0.04882104093127091\n","Cost after iteration 6570: 0.048782805657494335\n","Cost after iteration 6580: 0.048744657013541304\n","Cost after iteration 6590: 0.04870652209368093\n","Cost after iteration 6600: 0.0486685027714872\n","Cost after iteration 6610: 0.048630602544794836\n","Cost after iteration 6620: 0.04859276162935508\n","Cost after iteration 6630: 0.04855498497320429\n","Cost after iteration 6640: 0.04851726071420655\n","Cost after iteration 6650: 0.048479517323789176\n","Cost after iteration 6660: 0.04844187311175867\n","Cost after iteration 6670: 0.04840416364471019\n","Cost after iteration 6680: 0.048366448500336875\n","Cost after iteration 6690: 0.048328831094612924\n","Cost after iteration 6700: 0.04829133862127535\n","Cost after iteration 6710: 0.04825388487920678\n","Cost after iteration 6720: 0.04821659888548633\n","Cost after iteration 6730: 0.048179277146546046\n","Cost after iteration 6740: 0.04814203827260097\n","Cost after iteration 6750: 0.048104897269434184\n","Cost after iteration 6760: 0.04806780573359063\n","Cost after iteration 6770: 0.048030830002713285\n","Cost after iteration 6780: 0.04799390103472144\n","Cost after iteration 6790: 0.047957108650073396\n","Cost after iteration 6800: 0.04792041404067118\n","Cost after iteration 6810: 0.04788366944591655\n","Cost after iteration 6820: 0.04784694445139854\n","Cost after iteration 6830: 0.04781030630682276\n","Cost after iteration 6840: 0.047773676358556914\n","Cost after iteration 6850: 0.047737073989729324\n","Cost after iteration 6860: 0.04770055490939306\n","Cost after iteration 6870: 0.04766404159265267\n","Cost after iteration 6880: 0.04762755255828405\n","Cost after iteration 6890: 0.04759114161414635\n","Cost after iteration 6900: 0.047554838863924906\n","Cost after iteration 6910: 0.04751859591733912\n","Cost after iteration 6920: 0.04748241253104805\n","Cost after iteration 6930: 0.04744640324790959\n","Cost after iteration 6940: 0.04741035508126099\n","Cost after iteration 6950: 0.047374281456804175\n","Cost after iteration 6960: 0.04733828207841393\n","Cost after iteration 6970: 0.04730234981205407\n","Cost after iteration 6980: 0.04726647628763388\n","Cost after iteration 6990: 0.0472306041090039\n","Cost after iteration 7000: 0.047194765383615486\n","Cost after iteration 7010: 0.047158952264488824\n","Cost after iteration 7020: 0.04712328820834511\n","Cost after iteration 7030: 0.04708771358403771\n","Cost after iteration 7040: 0.04705224645988978\n","Cost after iteration 7050: 0.04701685983454179\n","Cost after iteration 7060: 0.04698152349012709\n","Cost after iteration 7070: 0.04694622960948658\n","Cost after iteration 7080: 0.04691093086575392\n","Cost after iteration 7090: 0.046875746369821156\n","Cost after iteration 7100: 0.04684055037937209\n","Cost after iteration 7110: 0.046805383617732114\n","Cost after iteration 7120: 0.046770308062898086\n","Cost after iteration 7130: 0.04673532336049808\n","Cost after iteration 7140: 0.046700356530913305\n","Cost after iteration 7150: 0.04666557583998162\n","Cost after iteration 7160: 0.046630873206909444\n","Cost after iteration 7170: 0.04659630852934158\n","Cost after iteration 7180: 0.046561832748204045\n","Cost after iteration 7190: 0.046527399124304356\n","Cost after iteration 7200: 0.04649295484014835\n","Cost after iteration 7210: 0.04645852206446494\n","Cost after iteration 7220: 0.04642422431800023\n","Cost after iteration 7230: 0.04639007412874248\n","Cost after iteration 7240: 0.0463559891911692\n","Cost after iteration 7250: 0.04632191381206856\n","Cost after iteration 7260: 0.04628792023602291\n","Cost after iteration 7270: 0.04625397313864545\n","Cost after iteration 7280: 0.046219987385402804\n","Cost after iteration 7290: 0.04618598584857336\n","Cost after iteration 7300: 0.046152097352132423\n","Cost after iteration 7310: 0.04611833841742613\n","Cost after iteration 7320: 0.04608465484980761\n","Cost after iteration 7330: 0.04605098799830998\n","Cost after iteration 7340: 0.046017435854163985\n","Cost after iteration 7350: 0.04598393037186659\n","Cost after iteration 7360: 0.04595043123664226\n","Cost after iteration 7370: 0.04591709325452793\n","Cost after iteration 7380: 0.04588381551399598\n","Cost after iteration 7390: 0.04585060270933749\n","Cost after iteration 7400: 0.04581749242152635\n","Cost after iteration 7410: 0.045784436031105415\n","Cost after iteration 7420: 0.045751393736637544\n","Cost after iteration 7430: 0.04571841952717807\n","Cost after iteration 7440: 0.04568545926752356\n","Cost after iteration 7450: 0.04565252986112393\n","Cost after iteration 7460: 0.045619688441359996\n","Cost after iteration 7470: 0.04558692138549342\n","Cost after iteration 7480: 0.04555412542897939\n","Cost after iteration 7490: 0.045521358842320114\n","Cost after iteration 7500: 0.04548867726165535\n","Cost after iteration 7510: 0.0454560287213317\n","Cost after iteration 7520: 0.04542343032186536\n","Cost after iteration 7530: 0.04539091671142179\n","Cost after iteration 7540: 0.045358499865580236\n","Cost after iteration 7550: 0.045326186984496225\n","Cost after iteration 7560: 0.045293976503858996\n","Cost after iteration 7570: 0.04526179973062751\n","Cost after iteration 7580: 0.04522957078084999\n","Cost after iteration 7590: 0.045197389672130675\n","Cost after iteration 7600: 0.04516520927576016\n","Cost after iteration 7610: 0.04513300857469652\n","Cost after iteration 7620: 0.045100913298779924\n","Cost after iteration 7630: 0.04506884180892996\n","Cost after iteration 7640: 0.045036750549210004\n","Cost after iteration 7650: 0.04500467538780552\n","Cost after iteration 7660: 0.044972626834999134\n","Cost after iteration 7670: 0.04494063139793205\n","Cost after iteration 7680: 0.044908746465084415\n","Cost after iteration 7690: 0.04487690106278457\n","Cost after iteration 7700: 0.04484503506341001\n","Cost after iteration 7710: 0.04481330936893904\n","Cost after iteration 7720: 0.04478172365497892\n","Cost after iteration 7730: 0.044750189236406016\n","Cost after iteration 7740: 0.044718700525293344\n","Cost after iteration 7750: 0.0446872149571732\n","Cost after iteration 7760: 0.04465575945340589\n","Cost after iteration 7770: 0.044624452966581414\n","Cost after iteration 7780: 0.04459323000781267\n","Cost after iteration 7790: 0.04456200204901341\n","Cost after iteration 7800: 0.04453085709373359\n","Cost after iteration 7810: 0.044499760701089734\n","Cost after iteration 7820: 0.044468766539017476\n","Cost after iteration 7830: 0.04443785204085972\n","Cost after iteration 7840: 0.04440685230654179\n","Cost after iteration 7850: 0.044375942043793866\n","Cost after iteration 7860: 0.044345014284943154\n","Cost after iteration 7870: 0.04431417871571515\n","Cost after iteration 7880: 0.044283409028659425\n","Cost after iteration 7890: 0.0442527186271701\n","Cost after iteration 7900: 0.0442221187619789\n","Cost after iteration 7910: 0.04419154867217883\n","Cost after iteration 7920: 0.04416093092227686\n","Cost after iteration 7930: 0.044130261909877636\n","Cost after iteration 7940: 0.04409961571757982\n","Cost after iteration 7950: 0.04406905293181731\n","Cost after iteration 7960: 0.044038447309898664\n","Cost after iteration 7970: 0.044007956256307816\n","Cost after iteration 7980: 0.04397751968912531\n","Cost after iteration 7990: 0.043947181242574476\n","Cost after iteration 8000: 0.04391696568496498\n","Cost after iteration 8010: 0.043886727423588526\n","Cost after iteration 8020: 0.0438565821450295\n","Cost after iteration 8030: 0.04382645439578686\n","Cost after iteration 8040: 0.04379636403322479\n","Cost after iteration 8050: 0.04376636825111514\n","Cost after iteration 8060: 0.04373638687504033\n","Cost after iteration 8070: 0.04370649844582241\n","Cost after iteration 8080: 0.043676621070371895\n","Cost after iteration 8090: 0.043646775705661604\n","Cost after iteration 8100: 0.043617030329835944\n","Cost after iteration 8110: 0.04358735196299052\n","Cost after iteration 8120: 0.043557721077030945\n","Cost after iteration 8130: 0.043528126396325945\n","Cost after iteration 8140: 0.043498585266054816\n","Cost after iteration 8150: 0.04346910923956881\n","Cost after iteration 8160: 0.04343961150209258\n","Cost after iteration 8170: 0.0434101929413604\n","Cost after iteration 8180: 0.04338080246798093\n","Cost after iteration 8190: 0.043351447312115814\n","Cost after iteration 8200: 0.04332212806464847\n","Cost after iteration 8210: 0.04329280831156144\n","Cost after iteration 8220: 0.043263449745837755\n","Cost after iteration 8230: 0.04323414838379741\n","Cost after iteration 8240: 0.043204925708329775\n","Cost after iteration 8250: 0.04317573028343975\n","Cost after iteration 8260: 0.043146616134104164\n","Cost after iteration 8270: 0.04311749375396539\n","Cost after iteration 8280: 0.04308840120136823\n","Cost after iteration 8290: 0.043059400592428285\n","Cost after iteration 8300: 0.04303041437105474\n","Cost after iteration 8310: 0.04300141705669811\n","Cost after iteration 8320: 0.04297232840009254\n","Cost after iteration 8330: 0.04294333688085299\n","Cost after iteration 8340: 0.04291437720524696\n","Cost after iteration 8350: 0.04288546977007894\n","Cost after iteration 8360: 0.042856640627766916\n","Cost after iteration 8370: 0.042827849514232054\n","Cost after iteration 8380: 0.042799095507731866\n","Cost after iteration 8390: 0.04277033968159925\n","Cost after iteration 8400: 0.042741629581132846\n","Cost after iteration 8410: 0.04271295257514758\n","Cost after iteration 8420: 0.04268435354318126\n","Cost after iteration 8430: 0.042655774352383616\n","Cost after iteration 8440: 0.04262721089156094\n","Cost after iteration 8450: 0.0425986589875558\n","Cost after iteration 8460: 0.0425701042865015\n","Cost after iteration 8470: 0.042541662586404065\n","Cost after iteration 8480: 0.042513292064588135\n","Cost after iteration 8490: 0.04248502778855155\n","Cost after iteration 8500: 0.04245681520060583\n","Cost after iteration 8510: 0.04242860936904442\n","Cost after iteration 8520: 0.042400557965793485\n","Cost after iteration 8530: 0.042372532395468135\n","Cost after iteration 8540: 0.04234454810000254\n","Cost after iteration 8550: 0.0423165971338021\n","Cost after iteration 8560: 0.042288661866448966\n","Cost after iteration 8570: 0.04226071435103451\n","Cost after iteration 8580: 0.04223278618812732\n","Cost after iteration 8590: 0.04220491325988032\n","Cost after iteration 8600: 0.04217706164890435\n","Cost after iteration 8610: 0.04214920937011493\n","Cost after iteration 8620: 0.04212142120393617\n","Cost after iteration 8630: 0.04209365618752431\n","Cost after iteration 8640: 0.042065979735898415\n","Cost after iteration 8650: 0.04203839469448972\n","Cost after iteration 8660: 0.04201084489087426\n","Cost after iteration 8670: 0.041983314499563294\n","Cost after iteration 8680: 0.04195579225092587\n","Cost after iteration 8690: 0.041928319039209135\n","Cost after iteration 8700: 0.04190090446210704\n","Cost after iteration 8710: 0.041873520915634545\n","Cost after iteration 8720: 0.04184623785237829\n","Cost after iteration 8730: 0.04181896108089406\n","Cost after iteration 8740: 0.041791651220894725\n","Cost after iteration 8750: 0.041764399833028036\n","Cost after iteration 8760: 0.041737217777420954\n","Cost after iteration 8770: 0.04171010517005272\n","Cost after iteration 8780: 0.0416829446967943\n","Cost after iteration 8790: 0.041655794584509534\n","Cost after iteration 8800: 0.04162869205235713\n","Cost after iteration 8810: 0.041601617894674835\n","Cost after iteration 8820: 0.041574524622010514\n","Cost after iteration 8830: 0.04154747060090618\n","Cost after iteration 8840: 0.04152045236238455\n","Cost after iteration 8850: 0.041493468862478275\n","Cost after iteration 8860: 0.04146651980927584\n","Cost after iteration 8870: 0.0414396282900004\n","Cost after iteration 8880: 0.04141282753093741\n","Cost after iteration 8890: 0.041386161588388225\n","Cost after iteration 8900: 0.041359538396802335\n","Cost after iteration 8910: 0.04133290065451227\n","Cost after iteration 8920: 0.041306281366844194\n","Cost after iteration 8930: 0.04127964769387434\n","Cost after iteration 8940: 0.04125318640123179\n","Cost after iteration 8950: 0.04122672674703665\n","Cost after iteration 8960: 0.0412003240831758\n","Cost after iteration 8970: 0.0411739670129731\n","Cost after iteration 8980: 0.041147604368496515\n","Cost after iteration 8990: 0.0411212482350707\n","Cost after iteration 9000: 0.04109490408209531\n","Cost after iteration 9010: 0.04106866272044835\n","Cost after iteration 9020: 0.041042457164240674\n","Cost after iteration 9030: 0.04101619241945156\n","Cost after iteration 9040: 0.04098991386210466\n","Cost after iteration 9050: 0.04096372571935516\n","Cost after iteration 9060: 0.040937611685038955\n","Cost after iteration 9070: 0.040911543218784416\n","Cost after iteration 9080: 0.040885505645820705\n","Cost after iteration 9090: 0.040859450274839365\n","Cost after iteration 9100: 0.04083342712834757\n","Cost after iteration 9110: 0.04080733862080165\n","Cost after iteration 9120: 0.04078131682991358\n","Cost after iteration 9130: 0.0407553782210366\n","Cost after iteration 9140: 0.040729435924587165\n","Cost after iteration 9150: 0.04070357338483485\n","Cost after iteration 9160: 0.04067770126138856\n","Cost after iteration 9170: 0.040651861043740675\n","Cost after iteration 9180: 0.04062607965734896\n","Cost after iteration 9190: 0.040600293498178355\n","Cost after iteration 9200: 0.04057452632332782\n","Cost after iteration 9210: 0.040548813064256166\n","Cost after iteration 9220: 0.040523199216171514\n","Cost after iteration 9230: 0.040497537712772404\n","Cost after iteration 9240: 0.04047190314825904\n","Cost after iteration 9250: 0.04044617596608698\n","Cost after iteration 9260: 0.04042044774330304\n","Cost after iteration 9270: 0.04039480405226077\n","Cost after iteration 9280: 0.04036923115210128\n","Cost after iteration 9290: 0.04034368730530951\n","Cost after iteration 9300: 0.04031812669709026\n","Cost after iteration 9310: 0.0402926513440333\n","Cost after iteration 9320: 0.040267204551328276\n","Cost after iteration 9330: 0.04024180192138447\n","Cost after iteration 9340: 0.04021639503514548\n","Cost after iteration 9350: 0.04019096350161033\n","Cost after iteration 9360: 0.04016566869767922\n","Cost after iteration 9370: 0.04014049813172572\n","Cost after iteration 9380: 0.040115370021249416\n","Cost after iteration 9390: 0.04009019180280225\n","Cost after iteration 9400: 0.0400650108727627\n","Cost after iteration 9410: 0.04003982732992808\n","Cost after iteration 9420: 0.04001473029229894\n","Cost after iteration 9430: 0.039989713452523286\n","Cost after iteration 9440: 0.03996477700467152\n","Cost after iteration 9450: 0.03993996341448922\n","Cost after iteration 9460: 0.03991516600181595\n","Cost after iteration 9470: 0.039890375816654264\n","Cost after iteration 9480: 0.03986561535342461\n","Cost after iteration 9490: 0.03984083742323599\n","Cost after iteration 9500: 0.03981605617192135\n","Cost after iteration 9510: 0.039791311898004174\n","Cost after iteration 9520: 0.03976659702515064\n","Cost after iteration 9530: 0.03974193843546011\n","Cost after iteration 9540: 0.03971736719673606\n","Cost after iteration 9550: 0.03969286650152835\n","Cost after iteration 9560: 0.03966841805940821\n","Cost after iteration 9570: 0.03964393353350673\n","Cost after iteration 9580: 0.03961936983964678\n","Cost after iteration 9590: 0.03959487053455397\n","Cost after iteration 9600: 0.03957040415627223\n","Cost after iteration 9610: 0.03954596092133058\n","Cost after iteration 9620: 0.03952153883949277\n","Cost after iteration 9630: 0.03949719425324053\n","Cost after iteration 9640: 0.03947289633104577\n","Cost after iteration 9650: 0.03944859228305188\n","Cost after iteration 9660: 0.03942438436725924\n","Cost after iteration 9670: 0.03940020595879439\n","Cost after iteration 9680: 0.03937602767775665\n","Cost after iteration 9690: 0.03935191093543407\n","Cost after iteration 9700: 0.039327793083915985\n","Cost after iteration 9710: 0.039303640191011104\n","Cost after iteration 9720: 0.0392795311016261\n","Cost after iteration 9730: 0.03925546588110947\n","Cost after iteration 9740: 0.039231421532497895\n","Cost after iteration 9750: 0.03920739445238705\n","Cost after iteration 9760: 0.039183340307697684\n","Cost after iteration 9770: 0.0391592810090269\n","Cost after iteration 9780: 0.03913526469162866\n","Cost after iteration 9790: 0.039111277358180624\n","Cost after iteration 9800: 0.03908742514483933\n","Cost after iteration 9810: 0.03906365179412659\n","Cost after iteration 9820: 0.03903994209625125\n","Cost after iteration 9830: 0.03901625938984521\n","Cost after iteration 9840: 0.0389925619942112\n","Cost after iteration 9850: 0.03896893776414864\n","Cost after iteration 9860: 0.038945357487682915\n","Cost after iteration 9870: 0.03892185661021686\n","Cost after iteration 9880: 0.038898356881040715\n","Cost after iteration 9890: 0.03887488169106026\n","Cost after iteration 9900: 0.03885147610762117\n","Cost after iteration 9910: 0.03882805422328819\n","Cost after iteration 9920: 0.038804641630571304\n","Cost after iteration 9930: 0.03878124289264182\n","Cost after iteration 9940: 0.038757813726010726\n","Cost after iteration 9950: 0.03873436861991123\n","Cost after iteration 9960: 0.038710947664490396\n","Cost after iteration 9970: 0.038687605065327015\n","Cost after iteration 9980: 0.0386643781729416\n","Cost after iteration 9990: 0.03864115403558932\n","Cost after iteration 10000: 0.03861791756464605\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9/8a8DTeu3zrNQAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7fa88bc9c978>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"TJyeSfcqf4FU","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}