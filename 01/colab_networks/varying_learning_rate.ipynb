{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of CuPy Neural Network",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "XLrcPMI9hMNl",
        "colab_type": "code",
        "outputId": "a368c1c9-599d-4991-8389-233b69f175a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "cell_type": "code",
      "source": [
        "!curl https://colab.chainer.org/install | sh -"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1379  100  1379    0     0   2965      0 --:--:-- --:--:-- --:--:--  2965\n",
            "+ apt -y -q install cuda-libraries-dev-9-2\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-9-2 is already the newest version (9.2.148-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "+ pip install -q cupy-cuda92  chainer \n",
            "+ set +ex\n",
            "Installation succeeded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3_n5ICc0hw33",
        "colab_type": "code",
        "outputId": "c5b92c9e-3ea0-438f-93a8-80d55cbd1231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MichalDanielDobrzanski/DeepLearningPython35"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DeepLearningPython35' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Huzc41d9ebnr",
        "colab_type": "code",
        "outputId": "f0158d5b-04a4-4013-d027-0f4e53bb01b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-oEFMBTpi153",
        "colab_type": "code",
        "outputId": "1af47e09-f970-4c02-c632-9a1b7280888f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install autograd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: autograd in /usr/local/lib/python3.6/dist-packages (1.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.6)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JffYydjtb8ye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from matplotlib import pyplot as plt\n",
        "import autograd.numpy as np_autograd\n",
        "from autograd import elementwise_grad as egrad\n",
        "import cupy as cp\n",
        "import pickle\n",
        "import math\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MFxa0eL5iJGQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %load mnist_loader.py\n",
        "\"\"\"\n",
        "mnist_loader\n",
        "~~~~~~~~~~~~\n",
        "A library to load the MNIST image data.  For details of the data\n",
        "structures that are returned, see the doc strings for ``load_data``\n",
        "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
        "function usually called by our neural network code.\n",
        "\"\"\"\n",
        "\n",
        "#### Libraries\n",
        "# Standard library\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
        "    the validation data, and the test data.\n",
        "    The ``training_data`` is returned as a tuple with two entries.\n",
        "    The first entry contains the actual training images.  This is a\n",
        "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
        "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
        "    pixels in a single MNIST image.\n",
        "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
        "    containing 50,000 entries.  Those entries are just the digit\n",
        "    values (0...9) for the corresponding images contained in the first\n",
        "    entry of the tuple.\n",
        "    The ``validation_data`` and ``test_data`` are similar, except\n",
        "    each contains only 10,000 images.\n",
        "    This is a nice data format, but for use in neural networks it's\n",
        "    helpful to modify the format of the ``training_data`` a little.\n",
        "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
        "    below.\n",
        "    \"\"\"\n",
        "    f = gzip.open('DeepLearningPython35/mnist.pkl.gz', 'rb')\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def load_data_wrapper():\n",
        "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
        "    test_data)``. Based on ``load_data``, but the format is more\n",
        "    convenient for use in our implementation of neural networks.\n",
        "    In particular, ``training_data`` is a list containing 50,000\n",
        "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
        "    containing the input image.  ``y`` is a 10-dimensional\n",
        "    numpy.ndarray representing the unit vector corresponding to the\n",
        "    correct digit for ``x``.\n",
        "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
        "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
        "    numpy.ndarry containing the input image, and ``y`` is the\n",
        "    corresponding classification, i.e., the digit values (integers)\n",
        "    corresponding to ``x``.\n",
        "    Obviously, this means we're using slightly different formats for\n",
        "    the training data and the validation / test data.  These formats\n",
        "    turn out to be the most convenient for use in our neural network\n",
        "    code.\"\"\"\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = (training_inputs, training_results) #zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = (validation_inputs, [vectorized_result(y) for y in va_d[1]]) #zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = (test_inputs, [vectorized_result(y) for y in te_d[1]]) #zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
        "    position and zeroes elsewhere.  This is used to convert a digit\n",
        "    (0...9) into a corresponding desired output from the neural\n",
        "    network.\"\"\"\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\n",
        "def load_data_arrays():\n",
        "    training_data, validation_data, test_data = load_data_wrapper()\n",
        "    training_data_X, training_data_Y = training_data\n",
        "    validation_data_X, validation_data_Y = validation_data\n",
        "    training_data_X, training_data_Y = training_data\n",
        "    training_data_X = cp.array(training_data_X)[:, :, 0].T\n",
        "    training_data_Y = cp.array(training_data_Y)[:, :, 0].T\n",
        "    validation_data_X, validation_data_Y = validation_data\n",
        "    validation_data_X = cp.array(validation_data_X)[:, :, 0].T\n",
        "    validation_data_Y = cp.array(validation_data_Y)[:, :, 0].T\n",
        "    test_data_X, test_data_Y = test_data\n",
        "    test_data_X = cp.array(test_data_X)[:, :, 0].T\n",
        "    test_data_Y = cp.array(test_data_Y)[:, :, 0].T\n",
        "    return training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MN-d_oj_iq4a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "    def __init__(self, layers_size_vector, activation_function, cost_function='cross-entropy',\n",
        "                 dropout_probabilities=None):\n",
        "        self.number_of_layers = len(layers_size_vector)\n",
        "        if type(activation_function) is list:\n",
        "            if self.number_of_layers != len(activation_function) + 1:\n",
        "                raise Exception(\"layer_size_vector & activation_function dimension mismatch\")\n",
        "            self.activation_function = activation_function\n",
        "        else:\n",
        "            self.activation_function = [activation_function] * self.number_of_layers\n",
        "        self.initialise_parameters(layers_size_vector)\n",
        "        self.layers_size_vector = layers_size_vector\n",
        "        self.cache = []\n",
        "        self.fitted = False\n",
        "        self.cost_function = cost_function\n",
        "        if dropout_probabilities:\n",
        "            if len(dropout_probabilities) != self.number_of_layers - 1:\n",
        "                raise Exception(\"dropout_probabilities length & number_of_layers dimension mismatch\")\n",
        "            self.dropout_probabilities = dict()\n",
        "            for key, dropout_probability in enumerate(dropout_probabilities, 1):\n",
        "                self.dropout_probabilities[key] = dropout_probability\n",
        "        else:\n",
        "            self.dropout_probabilities = dropout_probabilities\n",
        "\n",
        "    def initialise_parameters(self, layers_size_vector):\n",
        "        self.weights = dict()\n",
        "        self.bias = dict()\n",
        "        for i in range(1, self.number_of_layers):\n",
        "            #self.weights[i] = cp.random.randn(layers_size_vector[i], layers_size_vector[i - 1]) * cp.sqrt(2 / layers_size_vector[i - 1])  # He initialisation\n",
        "            self.weights[i] = cp.random.randn(layers_size_vector[i],layers_size_vector[i-1]) / cp.sqrt(layers_size_vector[i-1])\n",
        "            self.bias[i] = cp.zeros((layers_size_vector[i], 1))\n",
        "\n",
        "    def whole_output(self, A, dropout=False):\n",
        "        self.cache_A = dict()\n",
        "        self.cache_Z = dict()\n",
        "        self.cache_A[0] = A\n",
        "        for i in range(1, self.number_of_layers):\n",
        "            Z = self.linear_forward(A, i)\n",
        "            A = self.activation_function[i - 1](Z)\n",
        "            if bool(self.dropout_probabilities) & dropout:\n",
        "                if self.dropout_probabilities[i]!=0:\n",
        "                    A = A * self.dropout_mask[i] / self.dropout_probabilities[i]\n",
        "            self.cache_A[i] = A\n",
        "            self.cache_Z[i] = Z\n",
        "        return A\n",
        "\n",
        "    def linear_forward(self, previous_A, layer_no):\n",
        "        return cp.dot(self.weights[layer_no], previous_A) + self.bias[layer_no]\n",
        "\n",
        "    def predict(self, input_matrix):\n",
        "        output = self.whole_output(input_matrix)\n",
        "        if output.shape[0] == 1:\n",
        "            o = output > 0.5\n",
        "            return o\n",
        "        else:\n",
        "            return np.argmax(output, axis=0)\n",
        "\n",
        "    def cost_function_evaluation(self, X, Y, _lambda=0):\n",
        "        output = self.whole_output(X)\n",
        "        if self.cost_function == 'cross-entropy':\n",
        "            return cp.sum(-cp.multiply(Y, cp.log(output)) - cp.multiply((1 - Y), cp.log(1 - output))) / Y.shape[1]\n",
        "        elif self.cost_function == 'euclidean_distance':\n",
        "            return 1 / 2 * cp.sum(cp.power(Y - output, 2)) / Y.shape[1]\n",
        "\n",
        "    def output_layer_cost_derivative(self, output_matrix, Y):\n",
        "        if self.cost_function == 'cross-entropy':\n",
        "            return - (cp.divide(Y, output_matrix) - cp.divide(1 - Y, 1 - output_matrix))\n",
        "        elif self.cost_function == 'euclidean_distance':\n",
        "            return output_matrix - Y\n",
        "        else:\n",
        "            raise Exception(\"Wrong cost function name\")\n",
        "\n",
        "    def back_propagation(self, X, Y, regularisation_lambda=0):\n",
        "        self.cost_derivatives = dict()\n",
        "        self.weight_derivatives = dict()\n",
        "        self.bias_derivatives = dict()\n",
        "        dZ = self.output_layer_cost_derivative(self.whole_output(X), Y)\n",
        "        for i in reversed(range(1, self.number_of_layers)):\n",
        "            self.weight_derivatives[i] = (cp.dot(dZ, self.cache_A[i - 1].T) + regularisation_lambda * self.weights[i]) / X.shape[1]\n",
        "            self.bias_derivatives[i] = cp.sum(dZ, axis=1, keepdims=True) / X.shape[1]\n",
        "            self.cost_derivatives[i - 1] = cp.dot(self.weights[i].T, dZ)\n",
        "            if i > 1:\n",
        "                if self.dropout_probabilities:\n",
        "                    if self.dropout_probabilities[i-1]:\n",
        "                        self.cost_derivatives[i - 1] = self.cost_derivatives[i - 1] * self.dropout_mask[i - 1] / self.dropout_probabilities[i - 1]\n",
        "                dZ = self.cost_derivatives[i - 1] * self.activation_function[i - 2](self.cache_A[i - 1], grad=True)\n",
        "\n",
        "    def update_weights(self, learning_rate):\n",
        "        for i in range(1, self.number_of_layers):\n",
        "            self.weights[i] -= learning_rate * self.weight_derivatives[i]\n",
        "            self.bias[i] -= learning_rate * self.bias_derivatives[i]\n",
        "\n",
        "    def fit(self, X, Y, learning_rate, regularisation_lambda, epsilon, max_iteration_number=10000, min_max_normalization=False, validation_X = None, validation_Y = None):\n",
        "        self.training_costs = []\n",
        "        self.validation_costs = []\n",
        "        if not self.fitted:\n",
        "            if min_max_normalization:\n",
        "                X = self.min_max_scaler.fit_transform(X) - 0.5\n",
        "            previous_cost_function = float('inf')\n",
        "            number_of_training_examples = X.shape[1]\n",
        "            counter = 0\n",
        "#             while ((self.cost_function_evaluation(X, Y) / previous_cost_function <= epsilon) and ( counter < max_iteration_number)):\n",
        "            while counter<max_iteration_number:\n",
        "                previous_cost_function = self.cost_function_evaluation(X, Y)\n",
        "                if self.dropout_probabilities:\n",
        "                    self.dropout(number_of_training_examples)\n",
        "                self.whole_output(X, dropout=True)\n",
        "                self.back_propagation(X, Y, regularisation_lambda)\n",
        "                self.update_weights(learning_rate[counter])\n",
        "                counter += 1\n",
        "                if counter % 10 == 0:\n",
        "                    self.training_costs.append(self.cost_function_evaluation(X, Y))\n",
        "                    if (validation_X is not None) and (validation_Y is not None):\n",
        "                      self.validation_costs.append(self.cost_function_evaluation(validation_X, validation_Y))\n",
        "                    print(\"Cost after iteration {}: {}\".format(counter, self.training_costs[-1]))\n",
        "            self.fitted = True\n",
        "            self.pickle_network(regularisation_lambda, max_iteration_number)\n",
        "        else:\n",
        "            raise Exception(\"Neural network already fitted!\")\n",
        "\n",
        "    def set_weights(self, list_of_parameters):\n",
        "        for layer_no, parameters in enumerate(list_of_parameters):\n",
        "            self.weights[layer_no] = parameters[0]\n",
        "            self.bias[layer_no] = parameters[1]\n",
        "            \n",
        "    def pickle_network(self, regularisation_lambda, max_iteration_number):\n",
        "      network_name = str()\n",
        "      for i in range(1,self.number_of_layers):\n",
        "        network_name += str(self.layers_size_vector[i])+self.activation_function[1].__name__ + '_'\n",
        "      if self.dropout_probabilities:\n",
        "        network_name += str(list(self.dropout_probabilities.values())) #'_'.join(str(self.dropout_probabilities))\n",
        "      network_name += '_' + 'varying_lr'\n",
        "      network_name += '_' + str(regularisation_lambda)\n",
        "      network_name += '_' + str(max_iteration_number)\n",
        "      #network_name += 'random_init_with_sqrt'\n",
        "      with open('/content/gdrive/My Drive/'+network_name+'.pickle', 'wb') as f:\n",
        "        pickle.dump(self,f)\n",
        "\n",
        "    def dropout(self, number_of_examples):\n",
        "        self.dropout_mask = dict()\n",
        "        for i in range(1,self.number_of_layers):\n",
        "            if self.dropout_probabilities[i] != 0:\n",
        "                self.dropout_mask[i] = cp.random.rand(self.layers_size_vector[i], number_of_examples) > self.dropout_probabilities[i]\n",
        "\n",
        "\n",
        "def ReLU(x, grad=False):\n",
        "    if grad:\n",
        "      return x > 0\n",
        "#         return cp.int64(x > 0)\n",
        "    return x * (x > 0)\n",
        "\n",
        "\n",
        "def sigmoid(x, grad=False):\n",
        "    s = 1 / (1 + cp.exp(-x))\n",
        "    if grad:\n",
        "        return s * (1 - s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def softmax(x, grad=False):\n",
        "    def softmax_eval(x):\n",
        "        e_x = np_autograd.exp(x - np_autograd.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    softmax_eval_grad = egrad(softmax_eval)\n",
        "    if grad:\n",
        "        return softmax_eval_grad(x)\n",
        "    else:\n",
        "        return softmax_eval(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c50d1rZyi9Jr",
        "colab_type": "code",
        "outputId": "0ef44d6f-0a4c-4b3d-ee2c-0c20cd579110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "cell_type": "code",
      "source": [
        "activation_functions = [ReLU, ReLU, ReLU, ReLU, softmax]\n",
        "layers_size_vector = [784, 30, 30, 30, 30, 10]\n",
        "cost_function = 'euclidean_distance'\n",
        "# cost_function = 'cross-entropy'\n",
        "dropout_probabilities = [.1, .1, .1, .1, .1]\n",
        "learning_rate = .01\n",
        "number_of_iterations = 10000\n",
        "regularization_lambda = 5\n",
        "learning_rate = []\n",
        "for i in range(number_of_iterations):\n",
        "  learning_rate.append(0.0001 + 0.003 * (1/math.exp(1))**(i/2000))\n",
        "training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y = load_data_arrays()\n",
        "network = NeuralNetwork(layers_size_vector, activation_functions, cost_function, dropout_probabilities)\n",
        "network.fit(training_data_X, training_data_Y, learning_rate, regularization_lambda, 1, number_of_iterations, validation_X = validation_data_X, validation_Y = validation_data_Y)\n",
        "#network.fit(test_data_X, test_data_Y, learning_rate, regularization_lambda, 1, number_of_iterations)\n",
        "plt.plot(network.training_costs, 'o')\n",
        "plt.plot(network.validation_costs, 'ro')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-59872bf10889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactivation_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlayers_size_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcost_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'euclidean_distance'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# cost_function = 'cross-entropy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdropout_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;31m#[.1, .1, .1, .1, .1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ReLU' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TJyeSfcqf4FU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}