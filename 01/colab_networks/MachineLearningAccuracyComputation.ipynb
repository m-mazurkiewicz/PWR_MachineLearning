{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of MachineLearningAccuracyComputation.ipynb","version":"0.3.2","provenance":[{"file_id":"1C5nsAFh_yyiFdtHhudNl5egvfyFQ5cBl","timestamp":1542374756720}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"bWUOdlQnVXwn","colab_type":"code","colab":{}},"cell_type":"code","source":["!curl https://colab.chainer.org/install | sh -"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F8_6j0L0VjZ9","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install autograd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K0UpELb4WHD4","colab_type":"code","colab":{}},"cell_type":"code","source":["!git clone https://github.com/MichalDanielDobrzanski/DeepLearningPython35"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dwwn7zw2QxwH","colab_type":"code","outputId":"cf158750-2cd2-476c-b19d-1523026846d7","executionInfo":{"status":"ok","timestamp":1542368910532,"user_tz":-60,"elapsed":26075,"user":{"displayName":"Marcin Mazurkiewicz","photoUrl":"","userId":"07368942785876007571"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"cell_type":"code","source":["drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"bvCyp1CnRHyb","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","from sklearn.metrics import confusion_matrix\n","import pickle \n","import os\n","import numpy as np\n","from sklearn import preprocessing\n","from matplotlib import pyplot as plt\n","import autograd.numpy as np_autograd\n","from autograd import elementwise_grad as egrad\n","import cupy as cp\n","import pickle\n","import math\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uRgAc_BeVvvV","colab_type":"code","colab":{}},"cell_type":"code","source":["# %load mnist_loader.py\n","\"\"\"\n","mnist_loader\n","~~~~~~~~~~~~\n","A library to load the MNIST image data.  For details of the data\n","structures that are returned, see the doc strings for ``load_data``\n","and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n","function usually called by our neural network code.\n","\"\"\"\n","\n","#### Libraries\n","# Standard library\n","import pickle\n","import gzip\n","\n","# Third-party libraries\n","import numpy as np\n","\n","def load_data():\n","    \"\"\"Return the MNIST data as a tuple containing the training data,\n","    the validation data, and the test data.\n","    The ``training_data`` is returned as a tuple with two entries.\n","    The first entry contains the actual training images.  This is a\n","    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n","    numpy ndarray with 784 values, representing the 28 * 28 = 784\n","    pixels in a single MNIST image.\n","    The second entry in the ``training_data`` tuple is a numpy ndarray\n","    containing 50,000 entries.  Those entries are just the digit\n","    values (0...9) for the corresponding images contained in the first\n","    entry of the tuple.\n","    The ``validation_data`` and ``test_data`` are similar, except\n","    each contains only 10,000 images.\n","    This is a nice data format, but for use in neural networks it's\n","    helpful to modify the format of the ``training_data`` a little.\n","    That's done in the wrapper function ``load_data_wrapper()``, see\n","    below.\n","    \"\"\"\n","    f = gzip.open('DeepLearningPython35/mnist.pkl.gz', 'rb')\n","    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n","    f.close()\n","    return (training_data, validation_data, test_data)\n","\n","def load_data_wrapper():\n","    \"\"\"Return a tuple containing ``(training_data, validation_data,\n","    test_data)``. Based on ``load_data``, but the format is more\n","    convenient for use in our implementation of neural networks.\n","    In particular, ``training_data`` is a list containing 50,000\n","    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n","    containing the input image.  ``y`` is a 10-dimensional\n","    numpy.ndarray representing the unit vector corresponding to the\n","    correct digit for ``x``.\n","    ``validation_data`` and ``test_data`` are lists containing 10,000\n","    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n","    numpy.ndarry containing the input image, and ``y`` is the\n","    corresponding classification, i.e., the digit values (integers)\n","    corresponding to ``x``.\n","    Obviously, this means we're using slightly different formats for\n","    the training data and the validation / test data.  These formats\n","    turn out to be the most convenient for use in our neural network\n","    code.\"\"\"\n","    tr_d, va_d, te_d = load_data()\n","    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n","    training_results = [vectorized_result(y) for y in tr_d[1]]\n","    training_data = (training_inputs, training_results) #zip(training_inputs, training_results)\n","    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n","    validation_data = (validation_inputs, [vectorized_result(y) for y in va_d[1]]) #zip(validation_inputs, va_d[1])\n","    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n","    test_data = (test_inputs, [vectorized_result(y) for y in te_d[1]]) #zip(test_inputs, te_d[1])\n","    return (training_data, validation_data, test_data)\n","\n","def vectorized_result(j):\n","    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n","    position and zeroes elsewhere.  This is used to convert a digit\n","    (0...9) into a corresponding desired output from the neural\n","    network.\"\"\"\n","    e = np.zeros((10, 1))\n","    e[j] = 1.0\n","    return e\n","\n","\n","def load_data_arrays():\n","    training_data, validation_data, test_data = load_data_wrapper()\n","    training_data_X, training_data_Y = training_data\n","    validation_data_X, validation_data_Y = validation_data\n","    training_data_X, training_data_Y = training_data\n","    training_data_X = cp.array(training_data_X)[:, :, 0].T\n","    training_data_Y = cp.array(training_data_Y)[:, :, 0].T\n","    validation_data_X, validation_data_Y = validation_data\n","    validation_data_X = cp.array(validation_data_X)[:, :, 0].T\n","    validation_data_Y = cp.array(validation_data_Y)[:, :, 0].T\n","    test_data_X, test_data_Y = test_data\n","    test_data_X = cp.array(test_data_X)[:, :, 0].T\n","    test_data_Y = cp.array(test_data_Y)[:, :, 0].T\n","    return training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K_DMT3aKVrrJ","colab_type":"code","colab":{}},"cell_type":"code","source":["class NeuralNetwork:\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","\n","    def __init__(self, layers_size_vector, activation_function, cost_function='cross-entropy',\n","                 dropout_probabilities=None):\n","        self.number_of_layers = len(layers_size_vector)\n","        if type(activation_function) is list:\n","            if self.number_of_layers != len(activation_function) + 1:\n","                raise Exception(\"layer_size_vector & activation_function dimension mismatch\")\n","            self.activation_function = activation_function\n","        else:\n","            self.activation_function = [activation_function] * self.number_of_layers\n","        self.initialise_parameters(layers_size_vector)\n","        self.layers_size_vector = layers_size_vector\n","        self.cache = []\n","        self.fitted = False\n","        self.cost_function = cost_function\n","        if dropout_probabilities:\n","            if len(dropout_probabilities) != self.number_of_layers - 1:\n","                raise Exception(\"dropout_probabilities length & number_of_layers dimension mismatch\")\n","            self.dropout_probabilities = dict()\n","            for key, dropout_probability in enumerate(dropout_probabilities, 1):\n","                self.dropout_probabilities[key] = dropout_probability\n","        else:\n","            self.dropout_probabilities = dropout_probabilities\n","\n","    def initialise_parameters(self, layers_size_vector):\n","        self.weights = dict()\n","        self.bias = dict()\n","        for i in range(1, self.number_of_layers):\n","            #self.weights[i] = cp.random.randn(layers_size_vector[i], layers_size_vector[i - 1]) * cp.sqrt(2 / layers_size_vector[i - 1])  # He initialisation\n","            self.weights[i] = cp.random.randn(layers_size_vector[i], layers_size_vector[i - 1]) * 10 #/ cp.sqrt(layers_size_vector[i-1])\n","            self.bias[i] = cp.zeros((layers_size_vector[i], 1))\n","\n","    def whole_output(self, A, dropout=False):\n","        self.cache_A = dict()\n","        self.cache_Z = dict()\n","        self.cache_A[0] = A\n","        for i in range(1, self.number_of_layers):\n","            Z = self.linear_forward(A, i)\n","            A = self.activation_function[i - 1](Z)\n","            if bool(self.dropout_probabilities) & dropout:\n","                if self.dropout_probabilities[i]!=0:\n","                    A = A * self.dropout_mask[i] / self.dropout_probabilities[i]\n","            self.cache_A[i] = A\n","            self.cache_Z[i] = Z\n","        return A\n","\n","    def linear_forward(self, previous_A, layer_no):\n","        return cp.dot(self.weights[layer_no], previous_A) + self.bias[layer_no]\n","\n","    def predict(self, input_matrix):\n","        output = self.whole_output(input_matrix)\n","        if output.shape[0] == 1:\n","            o = output > 0.5\n","            return o\n","        else:\n","            return np.argmax(output, axis=0)\n","\n","    def cost_function_evaluation(self, X, Y, _lambda=0):\n","        output = self.whole_output(X)\n","        if self.cost_function == 'cross-entropy':\n","            return cp.sum(-cp.multiply(Y, cp.log(output)) - cp.multiply((1 - Y), cp.log(1 - output))) / Y.shape[1]\n","        elif self.cost_function == 'euclidean_distance':\n","            return 1 / 2 * cp.sum(cp.power(Y - output, 2)) / Y.shape[1]\n","\n","    def output_layer_cost_derivative(self, output_matrix, Y):\n","        if self.cost_function == 'cross-entropy':\n","            return - (cp.divide(Y, output_matrix) - cp.divide(1 - Y, 1 - output_matrix))\n","        elif self.cost_function == 'euclidean_distance':\n","            return output_matrix - Y\n","        else:\n","            raise Exception(\"Wrong cost function name\")\n","\n","    def back_propagation(self, X, Y, regularisation_lambda=0):\n","        self.cost_derivatives = dict()\n","        self.weight_derivatives = dict()\n","        self.bias_derivatives = dict()\n","        dZ = self.output_layer_cost_derivative(self.whole_output(X), Y)\n","        for i in reversed(range(1, self.number_of_layers)):\n","            self.weight_derivatives[i] = (cp.dot(dZ, self.cache_A[i - 1].T) + regularisation_lambda * self.weights[i]) / X.shape[1]\n","            self.bias_derivatives[i] = cp.sum(dZ, axis=1, keepdims=True) / X.shape[1]\n","            self.cost_derivatives[i - 1] = cp.dot(self.weights[i].T, dZ)\n","            if i > 1:\n","                if self.dropout_probabilities:\n","                    if self.dropout_probabilities[i-1]:\n","                        self.cost_derivatives[i - 1] = self.cost_derivatives[i - 1] * self.dropout_mask[i - 1] / self.dropout_probabilities[i - 1]\n","                dZ = self.cost_derivatives[i - 1] * self.activation_function[i - 2](self.cache_A[i - 1], grad=True)\n","\n","    def update_weights(self, learning_rate):\n","        for i in range(1, self.number_of_layers):\n","            self.weights[i] -= learning_rate * self.weight_derivatives[i]\n","            self.bias[i] -= learning_rate * self.bias_derivatives[i]\n","\n","    def fit(self, X, Y, learning_rate, regularisation_lambda, epsilon, max_iteration_number=10000, min_max_normalization=False, validation_X = None, validation_Y = None):\n","        self.training_costs = []\n","        self.validation_costs = []\n","        if not self.fitted:\n","            if min_max_normalization:\n","                X = self.min_max_scaler.fit_transform(X) - 0.5\n","            previous_cost_function = float('inf')\n","            number_of_training_examples = X.shape[1]\n","            counter = 0\n","#             while ((self.cost_function_evaluation(X, Y) / previous_cost_function <= epsilon) and ( counter < max_iteration_number)):\n","            while counter<max_iteration_number:\n","                previous_cost_function = self.cost_function_evaluation(X, Y)\n","                if self.dropout_probabilities:\n","                    self.dropout(number_of_training_examples)\n","                self.whole_output(X, dropout=True)\n","                self.back_propagation(X, Y, regularisation_lambda)\n","                self.update_weights(learning_rate)\n","                counter += 1\n","                if counter % 10 == 0:\n","                    self.training_costs.append(self.cost_function_evaluation(X, Y))\n","                    if (validation_X is not None) and (validation_Y is not None):\n","                      self.validation_costs.append(self.cost_function_evaluation(validation_X, validation_Y))\n","                    print(\"Cost after iteration {}: {}\".format(counter, self.training_costs[-1]))\n","            self.fitted = True\n","            self.pickle_network(learning_rate, regularisation_lambda, max_iteration_number)\n","        else:\n","            raise Exception(\"Neural network already fitted!\")\n","\n","    def set_weights(self, list_of_parameters):\n","        for layer_no, parameters in enumerate(list_of_parameters):\n","            self.weights[layer_no] = parameters[0]\n","            self.bias[layer_no] = parameters[1]\n","            \n","    def pickle_network(self, learning_rate, regularisation_lambda, max_iteration_number):\n","      network_name = str()\n","      for i in range(1,self.number_of_layers):\n","        network_name += str(self.layers_size_vector[i])+self.activation_function[1].__name__ + '_'\n","      if self.dropout_probabilities:\n","        network_name += str(list(self.dropout_probabilities.values())) #'_'.join(str(self.dropout_probabilities))\n","      network_name += '_' + str(learning_rate)\n","      network_name += '_' + str(regularisation_lambda)\n","      network_name += '_' + str(max_iteration_number)\n","      network_name += 'ordinary_random_init'\n","      with open('/content/gdrive/My Drive/'+network_name+'.pickle', 'wb') as f:\n","        pickle.dump(self,f)\n","\n","    def dropout(self, number_of_examples):\n","        self.dropout_mask = dict()\n","        for i in range(1,self.number_of_layers):\n","            if self.dropout_probabilities[i] != 0:\n","                self.dropout_mask[i] = cp.random.rand(self.layers_size_vector[i], number_of_examples) > self.dropout_probabilities[i]\n","\n","\n","def ReLU(x, grad=False):\n","    if grad:\n","      return x > 0\n","#         return cp.int64(x > 0)\n","    return x * (x > 0)\n","\n","\n","def sigmoid(x, grad=False):\n","    s = 1 / (1 + cp.exp(-x))\n","    if grad:\n","        return s * (1 - s)\n","    return s\n","\n","\n","def softmax(x, grad=False):\n","    def softmax_eval(x):\n","        e_x = np_autograd.exp(x - np_autograd.max(x))\n","        return e_x / e_x.sum(axis=0)\n","\n","    softmax_eval_grad = egrad(softmax_eval)\n","    if grad:\n","        return softmax_eval_grad(x)\n","    else:\n","        return softmax_eval(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LGdOYL8IXFZ-","colab_type":"code","colab":{}},"cell_type":"code","source":["training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y = load_data_arrays()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lo7oA79iXPf4","colab_type":"code","colab":{}},"cell_type":"code","source":["def accuracy(NeuralNetwork, testing_X, testing_Y):\n","  output = NeuralNetwork.whole_output(testing_X)\n","  predicted_Y = cp.argmax(output, axis=0)\n","  testing_Y = cp.argmax(testing_Y, axis=0)\n","  confusion_matrix_of_network = confusion_matrix(cp.asnumpy(testing_Y), cp.asnumpy(predicted_Y), labels=[0,1,2,3,4,5,6,7,8,9])\n","  accuracy_dict = {'average' : 0}\n","  for i in range(10):\n","    accuracy_dict[i] = confusion_matrix_of_network[i,i]/sum(confusion_matrix_of_network[i,:])\n","    accuracy_dict['average'] += confusion_matrix_of_network[i,i]\n","  accuracy_dict['average'] /= test_data_Y.shape[1]\n","  return accuracy_dict"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ifc9_o39mJFo","colab_type":"code","colab":{}},"cell_type":"code","source":["network_performance = dict()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cyn_fbFPRa-B","colab_type":"code","colab":{}},"cell_type":"code","source":["#path = '/content/drive/My Drive/MLNetworks'\n","path = '/content/drive/My Drive/NeuralNetworksPWr'\n","for file in os.listdir(path):\n","  with open(os.path.join(path, file), 'rb') as f:\n","    NN = pickle.load(f)\n","    network_performance[file] = accuracy(NN, test_data_X, test_data_Y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AHQkf-kpmaWJ","colab_type":"code","outputId":"4756470c-db28-4f1a-8a24-6d7207a01774","executionInfo":{"status":"ok","timestamp":1542373235432,"user_tz":-60,"elapsed":1261,"user":{"displayName":"Marcin Mazurkiewicz","photoUrl":"","userId":"07368942785876007571"}},"colab":{"base_uri":"https://localhost:8080/","height":387}},"cell_type":"code","source":["for network in network_performance.keys():\n","  print(network, network_performance[network]['average'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["200ReLU_100ReLU_60ReLU_30ReLU_10ReLU__varying_lr_0.5_10000.pickle 0.9092\n","200sigmoid_100sigmoid_60sigmoid_30sigmoid_10sigmoid_[0.1, 0.1, 0.1, 0.1, 0.1]_varying_lr_0.5_10000.pickle 0.3166\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_varying_lr_0_10000.pickle 0.9727\n","200sigmoid_100sigmoid_60sigmoid_30sigmoid_10sigmoid_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_0_10000ordinary_random_init.pickle 0.1629\n","30ReLU_30ReLU_30ReLU_30ReLU_10ReLU__varying_lr_5_10000.pickle 0.9497\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_varying_lr_0_10000random_init_with_sqrt.pickle 0.977\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU__0.003_0_10000.pickle 0.9477\n","200sigmoid_100sigmoid_60sigmoid_30sigmoid_10sigmoid__0.003_0_10000.pickle 0.1135\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_0_10000.pickle 0.9773\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU__0.003_0.5_10000.pickle 0.9443\n","200sigmoid_100sigmoid_60sigmoid_30sigmoid_10sigmoid__0.003_0.5_10000.pickle 0.1135\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_0.5_10000.pickle 0.9788\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_1_10000.pickle 0.9754\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_2_10000.pickle 0.9764\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_0.25_10000.pickle 0.9776\n","200softmax_10softmax__0.003_0_10000.pickle 0.9173\n","200softmax_10softmax_[0.25, 0.25]_0.003_0_10000.pickle 0.9337\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.2, 0.2, 0.2, 0.2, 0.2]_0.003_0_10000.pickle 0.9773\n","300ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_0_10000.pickle 0.9753\n","200ReLU_100ReLU_60ReLU_30ReLU_10ReLU_[0.25, 0.25, 0.25, 0.25, 0.25]_0.003_0_10000_init(v2).pickle 0.9769\n"],"name":"stdout"}]}]}