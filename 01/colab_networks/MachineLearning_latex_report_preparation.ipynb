{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MachineLearning_latex_report_preparation.ipynb","version":"0.3.2","provenance":[{"file_id":"15qe2AoZyGtOgdKGUSegsgwYN8nr3new1","timestamp":1542489746999},{"file_id":"1C5nsAFh_yyiFdtHhudNl5egvfyFQ5cBl","timestamp":1542374756720}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"bWUOdlQnVXwn","colab_type":"code","outputId":"1b0960fb-5bfd-4904-bc10-20692542aadc","executionInfo":{"status":"ok","timestamp":1542969666552,"user_tz":-60,"elapsed":75008,"user":{"displayName":"Maciej Falkiewicz","photoUrl":"","userId":"12525021891171762130"}},"colab":{"base_uri":"https://localhost:8080/","height":1252}},"cell_type":"code","source":["!curl https://colab.chainer.org/install | sh -"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1379  100  1379    0     0   7835      0 --:--:-- --:--:-- --:--:--  7835\n","+ apt -y -q install cuda-libraries-dev-9-2\n","Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","The following additional packages will be installed:\n","  cuda-cublas-dev-9-2 cuda-cufft-dev-9-2 cuda-curand-dev-9-2\n","  cuda-cusolver-dev-9-2 cuda-cusparse-dev-9-2 cuda-npp-dev-9-2\n","  cuda-nvgraph-dev-9-2 cuda-nvrtc-dev-9-2\n","The following NEW packages will be installed:\n","  cuda-cublas-dev-9-2 cuda-cufft-dev-9-2 cuda-curand-dev-9-2\n","  cuda-cusolver-dev-9-2 cuda-cusparse-dev-9-2 cuda-libraries-dev-9-2\n","  cuda-npp-dev-9-2 cuda-nvgraph-dev-9-2 cuda-nvrtc-dev-9-2\n","0 upgraded, 9 newly installed, 0 to remove and 5 not upgraded.\n","Need to get 332 MB of archives.\n","After this operation, 972 MB of additional disk space will be used.\n","Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-cublas-dev-9-2 9.2.148.1-1 [50.4 MB]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-cufft-dev-9-2 9.2.148-1 [106 MB]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-curand-dev-9-2 9.2.148-1 [57.8 MB]\n","Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-cusolver-dev-9-2 9.2.148-1 [8,184 kB]\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-cusparse-dev-9-2 9.2.148-1 [27.8 MB]\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-nvrtc-dev-9-2 9.2.148-1 [9,348 B]\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-nvgraph-dev-9-2 9.2.148-1 [30.1 MB]\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-npp-dev-9-2 9.2.148-1 [52.0 MB]\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1710/x86_64  cuda-libraries-dev-9-2 9.2.148-1 [2,598 B]\n","Fetched 332 MB in 7s (47.9 MB/s)\n","Selecting previously unselected package cuda-cublas-dev-9-2.\n","(Reading database ... 22298 files and directories currently installed.)\n","Preparing to unpack .../0-cuda-cublas-dev-9-2_9.2.148.1-1_amd64.deb ...\n","Unpacking cuda-cublas-dev-9-2 (9.2.148.1-1) ...\n","Selecting previously unselected package cuda-cufft-dev-9-2.\n","Preparing to unpack .../1-cuda-cufft-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-cufft-dev-9-2 (9.2.148-1) ...\n","Selecting previously unselected package cuda-curand-dev-9-2.\n","Preparing to unpack .../2-cuda-curand-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-curand-dev-9-2 (9.2.148-1) ...\n","Selecting previously unselected package cuda-cusolver-dev-9-2.\n","Preparing to unpack .../3-cuda-cusolver-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-cusolver-dev-9-2 (9.2.148-1) ...\n","Selecting previously unselected package cuda-cusparse-dev-9-2.\n","Preparing to unpack .../4-cuda-cusparse-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-cusparse-dev-9-2 (9.2.148-1) ...\n","Selecting previously unselected package cuda-nvrtc-dev-9-2.\n","Preparing to unpack .../5-cuda-nvrtc-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-nvrtc-dev-9-2 (9.2.148-1) ...\n","Selecting previously unselected package cuda-nvgraph-dev-9-2.\n","Preparing to unpack .../6-cuda-nvgraph-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-nvgraph-dev-9-2 (9.2.148-1) ...\n","Selecting previously unselected package cuda-npp-dev-9-2.\n","Preparing to unpack .../7-cuda-npp-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-npp-dev-9-2 (9.2.148-1) ...\n","Selecting previously unselected package cuda-libraries-dev-9-2.\n","Preparing to unpack .../8-cuda-libraries-dev-9-2_9.2.148-1_amd64.deb ...\n","Unpacking cuda-libraries-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-npp-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-curand-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-nvrtc-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-cusolver-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-cufft-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-cusparse-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-cublas-dev-9-2 (9.2.148.1-1) ...\n","Setting up cuda-nvgraph-dev-9-2 (9.2.148-1) ...\n","Setting up cuda-libraries-dev-9-2 (9.2.148-1) ...\n","+ pip install -q cupy-cuda92  chainer \n","+ set +ex\n","Installation succeeded!\n"],"name":"stdout"}]},{"metadata":{"id":"F8_6j0L0VjZ9","colab_type":"code","outputId":"9f2411c5-4c52-42da-bafc-d46be747d8a1","executionInfo":{"status":"ok","timestamp":1542969846073,"user_tz":-60,"elapsed":4833,"user":{"displayName":"Maciej Falkiewicz","photoUrl":"","userId":"12525021891171762130"}},"colab":{"base_uri":"https://localhost:8080/","height":199}},"cell_type":"code","source":["!pip install autograd"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting autograd\n","  Downloading https://files.pythonhosted.org/packages/08/7a/1ccee2a929d806ba3dbe632a196ad6a3f1423d6e261ae887e5fef2011420/autograd-1.2.tar.gz\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.6)\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n","Building wheels for collected packages: autograd\n","  Running setup.py bdist_wheel for autograd ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/72/6f/c2/40f130cca2c91f31d354bf72de282922479c09ce0b7853c4c5\n","Successfully built autograd\n","Installing collected packages: autograd\n","Successfully installed autograd-1.2\n"],"name":"stdout"}]},{"metadata":{"id":"K0UpELb4WHD4","colab_type":"code","outputId":"13a538f9-65fe-4c9e-bc37-8e8d543ca52b","executionInfo":{"status":"ok","timestamp":1542969897471,"user_tz":-60,"elapsed":2317,"user":{"displayName":"Maciej Falkiewicz","photoUrl":"","userId":"12525021891171762130"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["!git clone https://github.com/MichalDanielDobrzanski/DeepLearningPython35"],"execution_count":4,"outputs":[{"output_type":"stream","text":["fatal: destination path 'DeepLearningPython35' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"metadata":{"id":"bvCyp1CnRHyb","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","from sklearn.metrics import confusion_matrix\n","import pickle \n","import os\n","import numpy as np\n","from sklearn import preprocessing\n","from matplotlib import pyplot as plt\n","import autograd.numpy as np_autograd\n","from autograd import elementwise_grad as egrad\n","import cupy as cp\n","import pickle\n","import math\n","%matplotlib inline\n","from copy import copy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dwwn7zw2QxwH","colab_type":"code","outputId":"02a9ef87-41e4-45b1-c0f8-200a58a84575","executionInfo":{"status":"ok","timestamp":1542969935714,"user_tz":-60,"elapsed":31092,"user":{"displayName":"Maciej Falkiewicz","photoUrl":"","userId":"12525021891171762130"}},"colab":{"base_uri":"https://localhost:8080/","height":128}},"cell_type":"code","source":["drive.mount('/content/drive/')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"uRgAc_BeVvvV","colab_type":"code","colab":{}},"cell_type":"code","source":["# %load mnist_loader.py\n","\"\"\"\n","mnist_loader\n","~~~~~~~~~~~~\n","A library to load the MNIST image data.  For details of the data\n","structures that are returned, see the doc strings for ``load_data``\n","and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n","function usually called by our neural network code.\n","\"\"\"\n","\n","#### Libraries\n","# Standard library\n","import pickle\n","import gzip\n","\n","# Third-party libraries\n","import numpy as np\n","\n","def load_data():\n","    \"\"\"Return the MNIST data as a tuple containing the training data,\n","    the validation data, and the test data.\n","    The ``training_data`` is returned as a tuple with two entries.\n","    The first entry contains the actual training images.  This is a\n","    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n","    numpy ndarray with 784 values, representing the 28 * 28 = 784\n","    pixels in a single MNIST image.\n","    The second entry in the ``training_data`` tuple is a numpy ndarray\n","    containing 50,000 entries.  Those entries are just the digit\n","    values (0...9) for the corresponding images contained in the first\n","    entry of the tuple.\n","    The ``validation_data`` and ``test_data`` are similar, except\n","    each contains only 10,000 images.\n","    This is a nice data format, but for use in neural networks it's\n","    helpful to modify the format of the ``training_data`` a little.\n","    That's done in the wrapper function ``load_data_wrapper()``, see\n","    below.\n","    \"\"\"\n","    f = gzip.open('DeepLearningPython35/mnist.pkl.gz', 'rb')\n","    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n","    f.close()\n","    return (training_data, validation_data, test_data)\n","\n","def load_data_wrapper():\n","    \"\"\"Return a tuple containing ``(training_data, validation_data,\n","    test_data)``. Based on ``load_data``, but the format is more\n","    convenient for use in our implementation of neural networks.\n","    In particular, ``training_data`` is a list containing 50,000\n","    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n","    containing the input image.  ``y`` is a 10-dimensional\n","    numpy.ndarray representing the unit vector corresponding to the\n","    correct digit for ``x``.\n","    ``validation_data`` and ``test_data`` are lists containing 10,000\n","    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n","    numpy.ndarry containing the input image, and ``y`` is the\n","    corresponding classification, i.e., the digit values (integers)\n","    corresponding to ``x``.\n","    Obviously, this means we're using slightly different formats for\n","    the training data and the validation / test data.  These formats\n","    turn out to be the most convenient for use in our neural network\n","    code.\"\"\"\n","    tr_d, va_d, te_d = load_data()\n","    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n","    training_results = [vectorized_result(y) for y in tr_d[1]]\n","    training_data = (training_inputs, training_results) #zip(training_inputs, training_results)\n","    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n","    validation_data = (validation_inputs, [vectorized_result(y) for y in va_d[1]]) #zip(validation_inputs, va_d[1])\n","    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n","    test_data = (test_inputs, [vectorized_result(y) for y in te_d[1]]) #zip(test_inputs, te_d[1])\n","    return (training_data, validation_data, test_data)\n","\n","def vectorized_result(j):\n","    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n","    position and zeroes elsewhere.  This is used to convert a digit\n","    (0...9) into a corresponding desired output from the neural\n","    network.\"\"\"\n","    e = np.zeros((10, 1))\n","    e[j] = 1.0\n","    return e\n","\n","\n","def load_data_arrays():\n","    training_data, validation_data, test_data = load_data_wrapper()\n","    training_data_X, training_data_Y = training_data\n","    validation_data_X, validation_data_Y = validation_data\n","    training_data_X, training_data_Y = training_data\n","    training_data_X = cp.array(training_data_X)[:, :, 0].T\n","    training_data_Y = cp.array(training_data_Y)[:, :, 0].T\n","    validation_data_X, validation_data_Y = validation_data\n","    validation_data_X = cp.array(validation_data_X)[:, :, 0].T\n","    validation_data_Y = cp.array(validation_data_Y)[:, :, 0].T\n","    test_data_X, test_data_Y = test_data\n","    test_data_X = cp.array(test_data_X)[:, :, 0].T\n","    test_data_Y = cp.array(test_data_Y)[:, :, 0].T\n","    return training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K_DMT3aKVrrJ","colab_type":"code","colab":{}},"cell_type":"code","source":["class NeuralNetwork:\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","\n","    def __init__(self, layers_size_vector, activation_function, cost_function='cross-entropy',\n","                 dropout_probabilities=None):\n","        self.number_of_layers = len(layers_size_vector)\n","        if type(activation_function) is list:\n","            if self.number_of_layers != len(activation_function) + 1:\n","                raise Exception(\"layer_size_vector & activation_function dimension mismatch\")\n","            self.activation_function = activation_function\n","        else:\n","            self.activation_function = [activation_function] * self.number_of_layers\n","        self.initialise_parameters(layers_size_vector)\n","        self.layers_size_vector = layers_size_vector\n","        self.cache = []\n","        self.fitted = False\n","        self.cost_function = cost_function\n","        if dropout_probabilities:\n","            if len(dropout_probabilities) != self.number_of_layers - 1:\n","                raise Exception(\"dropout_probabilities length & number_of_layers dimension mismatch\")\n","            self.dropout_probabilities = dict()\n","            for key, dropout_probability in enumerate(dropout_probabilities, 1):\n","                self.dropout_probabilities[key] = dropout_probability\n","        else:\n","            self.dropout_probabilities = dropout_probabilities\n","\n","    def initialise_parameters(self, layers_size_vector):\n","        self.weights = dict()\n","        self.bias = dict()\n","        for i in range(1, self.number_of_layers):\n","            #self.weights[i] = cp.random.randn(layers_size_vector[i], layers_size_vector[i - 1]) * cp.sqrt(2 / layers_size_vector[i - 1])  # He initialisation\n","            self.weights[i] = cp.random.randn(layers_size_vector[i], layers_size_vector[i - 1]) * 10 #/ cp.sqrt(layers_size_vector[i-1])\n","            self.bias[i] = cp.zeros((layers_size_vector[i], 1))\n","\n","    def whole_output(self, A, dropout=False):\n","        self.cache_A = dict()\n","        self.cache_Z = dict()\n","        self.cache_A[0] = A\n","        for i in range(1, self.number_of_layers):\n","            Z = self.linear_forward(A, i)\n","            A = self.activation_function[i - 1](Z)\n","            if bool(self.dropout_probabilities) & dropout:\n","                if self.dropout_probabilities[i]!=0:\n","                    A = A * self.dropout_mask[i] / self.dropout_probabilities[i]\n","            self.cache_A[i] = A\n","            self.cache_Z[i] = Z\n","        return A\n","\n","    def linear_forward(self, previous_A, layer_no):\n","        return cp.dot(self.weights[layer_no], previous_A) + self.bias[layer_no]\n","\n","    def predict(self, input_matrix):\n","        output = self.whole_output(input_matrix)\n","        if output.shape[0] == 1:\n","            o = output > 0.5\n","            return o\n","        else:\n","            return np.argmax(output, axis=0)\n","\n","    def cost_function_evaluation(self, X, Y, _lambda=0):\n","        output = self.whole_output(X)\n","        if self.cost_function == 'cross-entropy':\n","            return cp.sum(-cp.multiply(Y, cp.log(output)) - cp.multiply((1 - Y), cp.log(1 - output))) / Y.shape[1]\n","        elif self.cost_function == 'euclidean_distance':\n","            return 1 / 2 * cp.sum(cp.power(Y - output, 2)) / Y.shape[1]\n","\n","    def output_layer_cost_derivative(self, output_matrix, Y):\n","        if self.cost_function == 'cross-entropy':\n","            return - (cp.divide(Y, output_matrix) - cp.divide(1 - Y, 1 - output_matrix))\n","        elif self.cost_function == 'euclidean_distance':\n","            return output_matrix - Y\n","        else:\n","            raise Exception(\"Wrong cost function name\")\n","\n","    def back_propagation(self, X, Y, regularisation_lambda=0):\n","        self.cost_derivatives = dict()\n","        self.weight_derivatives = dict()\n","        self.bias_derivatives = dict()\n","        dZ = self.output_layer_cost_derivative(self.whole_output(X), Y)\n","        for i in reversed(range(1, self.number_of_layers)):\n","            self.weight_derivatives[i] = (cp.dot(dZ, self.cache_A[i - 1].T) + regularisation_lambda * self.weights[i]) / X.shape[1]\n","            self.bias_derivatives[i] = cp.sum(dZ, axis=1, keepdims=True) / X.shape[1]\n","            self.cost_derivatives[i - 1] = cp.dot(self.weights[i].T, dZ)\n","            if i > 1:\n","                if self.dropout_probabilities:\n","                    if self.dropout_probabilities[i-1]:\n","                        self.cost_derivatives[i - 1] = self.cost_derivatives[i - 1] * self.dropout_mask[i - 1] / self.dropout_probabilities[i - 1]\n","                dZ = self.cost_derivatives[i - 1] * self.activation_function[i - 2](self.cache_A[i - 1], grad=True)\n","\n","    def update_weights(self, learning_rate):\n","        for i in range(1, self.number_of_layers):\n","            self.weights[i] -= learning_rate * self.weight_derivatives[i]\n","            self.bias[i] -= learning_rate * self.bias_derivatives[i]\n","\n","    def fit(self, X, Y, learning_rate, regularisation_lambda, epsilon, max_iteration_number=10000, min_max_normalization=False, validation_X = None, validation_Y = None):\n","        self.training_costs = []\n","        self.validation_costs = []\n","        if not self.fitted:\n","            if min_max_normalization:\n","                X = self.min_max_scaler.fit_transform(X) - 0.5\n","            previous_cost_function = float('inf')\n","            number_of_training_examples = X.shape[1]\n","            counter = 0\n","#             while ((self.cost_function_evaluation(X, Y) / previous_cost_function <= epsilon) and ( counter < max_iteration_number)):\n","            while counter<max_iteration_number:\n","                previous_cost_function = self.cost_function_evaluation(X, Y)\n","                if self.dropout_probabilities:\n","                    self.dropout(number_of_training_examples)\n","                self.whole_output(X, dropout=True)\n","                self.back_propagation(X, Y, regularisation_lambda)\n","                self.update_weights(learning_rate)\n","                counter += 1\n","                if counter % 10 == 0:\n","                    self.training_costs.append(self.cost_function_evaluation(X, Y))\n","                    if (validation_X is not None) and (validation_Y is not None):\n","                      self.validation_costs.append(self.cost_function_evaluation(validation_X, validation_Y))\n","                    print(\"Cost after iteration {}: {}\".format(counter, self.training_costs[-1]))\n","            self.fitted = True\n","            self.pickle_network(learning_rate, regularisation_lambda, max_iteration_number)\n","        else:\n","            raise Exception(\"Neural network already fitted!\")\n","\n","    def set_weights(self, list_of_parameters):\n","        for layer_no, parameters in enumerate(list_of_parameters):\n","            self.weights[layer_no] = parameters[0]\n","            self.bias[layer_no] = parameters[1]\n","            \n","    def pickle_network(self, learning_rate, regularisation_lambda, max_iteration_number):\n","      network_name = str()\n","      for i in range(1,self.number_of_layers):\n","        network_name += str(self.layers_size_vector[i])+self.activation_function[1].__name__ + '_'\n","      if self.dropout_probabilities:\n","        network_name += str(list(self.dropout_probabilities.values())) #'_'.join(str(self.dropout_probabilities))\n","      network_name += '_' + str(learning_rate)\n","      network_name += '_' + str(regularisation_lambda)\n","      network_name += '_' + str(max_iteration_number)\n","      network_name += 'ordinary_random_init'\n","      with open('/content/gdrive/My Drive/'+network_name+'.pickle', 'wb') as f:\n","        pickle.dump(self,f)\n","\n","    def dropout(self, number_of_examples):\n","        self.dropout_mask = dict()\n","        for i in range(1,self.number_of_layers):\n","            if self.dropout_probabilities[i] != 0:\n","                self.dropout_mask[i] = cp.random.rand(self.layers_size_vector[i], number_of_examples) > self.dropout_probabilities[i]\n","\n","\n","def ReLU(x, grad=False):\n","    if grad:\n","      return x > 0\n","#         return cp.int64(x > 0)\n","    return x * (x > 0)\n","\n","\n","def sigmoid(x, grad=False):\n","    s = 1 / (1 + cp.exp(-x))\n","    if grad:\n","        return s * (1 - s)\n","    return s\n","\n","\n","def softmax(x, grad=False):\n","    def softmax_eval(x):\n","        e_x = np_autograd.exp(x - np_autograd.max(x))\n","        return e_x / e_x.sum(axis=0)\n","\n","    softmax_eval_grad = egrad(softmax_eval)\n","    if grad:\n","        return softmax_eval_grad(x)\n","    else:\n","        return softmax_eval(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LGdOYL8IXFZ-","colab_type":"code","colab":{}},"cell_type":"code","source":["training_data_X, training_data_Y, validation_data_X, validation_data_Y, test_data_X, test_data_Y = load_data_arrays()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lo7oA79iXPf4","colab_type":"code","colab":{}},"cell_type":"code","source":["def accuracy(NeuralNetwork, testing_X, testing_Y):\n","  output = NeuralNetwork.whole_output(testing_X)\n","  predicted_Y = cp.argmax(output, axis=0)\n","  testing_Y = cp.argmax(testing_Y, axis=0)\n","  confusion_matrix_of_network = confusion_matrix(cp.asnumpy(testing_Y), cp.asnumpy(predicted_Y), labels=[0,1,2,3,4,5,6,7,8,9])\n","  accuracy_dict = {'average' : 0}\n","  for i in range(10):\n","    accuracy_dict[i] = confusion_matrix_of_network[i,i]/sum(confusion_matrix_of_network[i,:])\n","    accuracy_dict['average'] += confusion_matrix_of_network[i,i]\n","  accuracy_dict['average'] /= test_data_Y.shape[1]\n","  return accuracy_dict"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Cyn_fbFPRa-B","colab_type":"code","colab":{}},"cell_type":"code","source":["path_list = ['/content/drive/My Drive/NeuralNetworksPWr','/content/drive/My Drive/MLNetworks']\n","for path in path_list:\n","  for file in os.listdir(path):\n","    if True:\n","      network_path = os.path.join('/content/drive/My Drive/NN_summary', file.replace('.pickle',''))\n","      os.makedirs(network_path, exist_ok=True)\n","      if 'varying_lr' in file:\n","        learning_rate = 'varying_lr'\n","      else:\n","        learning_rate = 0.003\n","      with open(network_path +'/learning_rate.pickle', 'wb') as f:\n","        pickle.dump(learning_rate, f)\n","      file_split = file.split('_')\n","      for key, value in enumerate(file_split):\n","        if '10000' in value:\n","          regularization = file_split[key-1]\n","      with open(network_path +'/regularization.pickle', 'wb') as f:\n","        pickle.dump(regularization, f)\n","      if ('sqrt' in file) or ('v2' in file):\n","        initialization = 'random normal normalised'\n","      elif 'ordinary' in file:\n","        initialization = 'random normal'\n","      else:\n","        initialization = 'He initialization'\n","      with open(network_path +'/initialization.pickle', 'wb') as f:\n","        pickle.dump(initialization, f)\n","      with open(os.path.join(path, file), 'rb') as f:\n","        NN = pickle.load(f)\n","        accuracy_dict = accuracy(NN, test_data_X, test_data_Y)\n","        with open(network_path +'/accuracy_dict.pickle', 'wb') as f:\n","          pickle.dump(accuracy_dict, f)\n","        x = np.arange(0,10000, step=10)\n","        plt.figure(figsize=(16,10), dpi=300)\n","        train, = plt.plot(x,NN.training_costs, 'ob', markersize = 3)\n","        validation, = plt.plot(x,NN.validation_costs, 'or', markersize = 3)\n","        lgnd = plt.legend((train, validation), ('Train set', 'Validation set'), prop={'size': 20})\n","        lgnd.legendHandles[0]._legmarker.set_markersize(8)\n","        lgnd.legendHandles[1]._legmarker.set_markersize(8)\n","        plt.xlim(-1,10001)\n","        plt.savefig(network_path + '/learning_curves.pdf', bbox_inches = 'tight')\n","        plt.close()\n","        plt.figure(figsize=(16,10), dpi=300)\n","        train, = plt.plot(x,NN.training_costs, 'ob', markersize = 3)\n","        validation, = plt.plot(x,NN.validation_costs, 'or', markersize = 3)\n","        lgnd = plt.legend((train, validation), ('Train set', 'Validation set'), prop={'size': 20})\n","        lgnd.legendHandles[0]._legmarker.set_markersize(8)\n","        lgnd.legendHandles[1]._legmarker.set_markersize(8)\n","        plt.ylim(0,1)\n","        plt.xlim(-1,10001)\n","        plt.savefig(network_path + '/learning_curves01.pdf', bbox_inches = 'tight')\n","        plt.close()\n","        network_architecture = dict()\n","        for key, activation_function in enumerate(NN.activation_function, 1):\n","          layer = dict()\n","          layer['size'] = NN.layers_size_vector[key]\n","          layer['activation_function'] = activation_function.__name__\n","          if NN.dropout_probabilities:\n","            layer['dropout'] = NN.dropout_probabilities[key]\n","          else:\n","            layer['dropout'] = 0\n","          network_architecture[key] = copy(layer)\n","        with open(network_path +'/network_architecture.pickle', 'wb') as f:\n","          pickle.dump(network_architecture, f)"],"execution_count":0,"outputs":[]}]}